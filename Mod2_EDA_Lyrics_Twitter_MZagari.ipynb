{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b184b",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "360c0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import html\n",
    "import string\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71fedf",
   "metadata": {},
   "source": [
    "# Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well.\n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3d05211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/users/drzag/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588d597",
   "metadata": {},
   "source": [
    "## Function to Extract Followers into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f41786fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aboveandbeyond_followers.txt', 'didoofficial_followers.txt']\n"
     ]
    }
   ],
   "source": [
    "def follower_import(filenum):\n",
    "    fol = []\n",
    "    raw_file = os.listdir('C:/Users/drzag/twitter')[filenum]\n",
    "    file = ('C:/Users/drzag/twitter/'+raw_file)\n",
    "    fol = pd.read_csv(file, sep=\"\\t\", header=None)\n",
    "    #CODE NOTE: \"\\t\" was required to correctly read the file.\n",
    "    fol.columns = [\"name\", \"handle\", \"id_num\", \"loc\", \"date_joined\", \"followers\", \"friends\", \"desc\"]\n",
    "    fol['desc'] = fol['desc'].apply(str)\n",
    "    fol.drop(fol[fol['desc'] == 'nan'].index, inplace = True)\n",
    "    fol['desc_len'] = fol['desc'].str.len()\n",
    "    return fol\n",
    "\n",
    "print(os.listdir('C:/Users/drzag/twitter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14ade1",
   "metadata": {},
   "source": [
    "## Import Dido Follower Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fade82c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60699 entries, 0 to 100893\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name         60686 non-null  object\n",
      " 1   handle       60699 non-null  object\n",
      " 2   id_num       60699 non-null  int64 \n",
      " 3   loc          45836 non-null  object\n",
      " 4   date_joined  60699 non-null  object\n",
      " 5   followers    60699 non-null  int64 \n",
      " 6   friends      60699 non-null  int64 \n",
      " 7   desc         60699 non-null  object\n",
      " 8   desc_len     60699 non-null  int64 \n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 4.6+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>handle</th>\n",
       "      <th>id_num</th>\n",
       "      <th>loc</th>\n",
       "      <th>date_joined</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>desc</th>\n",
       "      <th>desc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RenElDepre</td>\n",
       "      <td>ElRenirey</td>\n",
       "      <td>1368982010920898561</td>\n",
       "      <td>Santiago, Chile</td>\n",
       "      <td>2021-03-08 17:48:50+00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>195</td>\n",
       "      <td>Lo que me Apasiona 😇 Lo que me Divierte😎 Lo qu...</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrackinLeftF⚽⚽t</td>\n",
       "      <td>DavidMarcusBen1</td>\n",
       "      <td>1307948922191609857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-21 07:45:08+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "      <td>'Flawed Genuis'</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W.K BRIAN</td>\n",
       "      <td>WKBrian7</td>\n",
       "      <td>1483097247604609025</td>\n",
       "      <td>JINJA</td>\n",
       "      <td>2022-01-17 15:22:29+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>SAXOPHONE PLAYER,GOSPEL MUSIC WRITER,SINGER AN...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name           handle               id_num              loc  \\\n",
       "0       RenElDepre        ElRenirey  1368982010920898561  Santiago, Chile   \n",
       "3  CrackinLeftF⚽⚽t  DavidMarcusBen1  1307948922191609857              NaN   \n",
       "4        W.K BRIAN         WKBrian7  1483097247604609025            JINJA   \n",
       "\n",
       "                 date_joined  followers  friends  \\\n",
       "0  2021-03-08 17:48:50+00:00         10      195   \n",
       "3  2020-09-21 07:45:08+00:00          6       57   \n",
       "4  2022-01-17 15:22:29+00:00          0        3   \n",
       "\n",
       "                                                desc  desc_len  \n",
       "0  Lo que me Apasiona 😇 Lo que me Divierte😎 Lo qu...        81  \n",
       "3                                    'Flawed Genuis'        15  \n",
       "4  SAXOPHONE PLAYER,GOSPEL MUSIC WRITER,SINGER AN...        56  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dido_fol = follower_import(1)  \n",
    "dido_fol.describe().T\n",
    "print(dido_fol.info())\n",
    "dido_fol.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274a382",
   "metadata": {},
   "source": [
    "## Import Above & Beyond Follower Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ddf6c047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 65125 entries, 0 to 100889\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name         65098 non-null  object\n",
      " 1   handle       65125 non-null  object\n",
      " 2   id_num       65125 non-null  int64 \n",
      " 3   loc          44805 non-null  object\n",
      " 4   date_joined  65125 non-null  object\n",
      " 5   followers    65125 non-null  int64 \n",
      " 6   friends      65125 non-null  int64 \n",
      " 7   desc         65125 non-null  object\n",
      " 8   desc_len     65125 non-null  int64 \n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 5.0+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>handle</th>\n",
       "      <th>id_num</th>\n",
       "      <th>loc</th>\n",
       "      <th>date_joined</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>desc</th>\n",
       "      <th>desc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DjSpace.booty</td>\n",
       "      <td>djspace_booty</td>\n",
       "      <td>1568710404314124288</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>2022-09-10 21:18:25+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>LA DJ with a passion for house music &amp; a nice ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Felipe Rodrigues Figueiredo</td>\n",
       "      <td>FelipeR87147318</td>\n",
       "      <td>1568656781945061381</td>\n",
       "      <td>São José do Rio Preto, Brasil</td>\n",
       "      <td>2022-09-10 17:46:38+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>331</td>\n",
       "      <td>See Off</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Warner Powers</td>\n",
       "      <td>itswarnerpowers</td>\n",
       "      <td>1568338123477721089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-09-09 20:38:49+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>DJ &amp; Producer</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name           handle               id_num  \\\n",
       "0                DjSpace.booty    djspace_booty  1568710404314124288   \n",
       "5  Felipe Rodrigues Figueiredo  FelipeR87147318  1568656781945061381   \n",
       "6                Warner Powers  itswarnerpowers  1568338123477721089   \n",
       "\n",
       "                             loc                date_joined  followers  \\\n",
       "0                Los Angeles, CA  2022-09-10 21:18:25+00:00          3   \n",
       "5  São José do Rio Preto, Brasil  2022-09-10 17:46:38+00:00          3   \n",
       "6                            NaN  2022-09-09 20:38:49+00:00          1   \n",
       "\n",
       "   friends                                               desc  desc_len  \n",
       "0       35  LA DJ with a passion for house music & a nice ...        50  \n",
       "5      331                                            See Off         7  \n",
       "6       54                                      DJ & Producer        13  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_fol = follower_import(0)  \n",
    "ab_fol.describe().T\n",
    "print(ab_fol.info())\n",
    "ab_fol.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6708bc",
   "metadata": {},
   "source": [
    "### Import Dido Song Titles and Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "429fd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relative path to folder containing the text files\n",
    "\n",
    "files_folder = \"/users/drzag/lyrics/dido\"\n",
    "files = []\n",
    "dido_files_df = pd.DataFrame()\n",
    "\n",
    "# Create a dataframe list and concatenate\n",
    "\n",
    "files = [pd.read_csv(file, delimiter='\\t') for file in glob.glob(os.path.join(files_folder ,\"*.txt\"))]\n",
    "dido_files_df = pd.concat(files, axis = 1)\n",
    "\n",
    "# EXTRACT TITLES TO NEW DF\n",
    "dido_titles = []\n",
    "dido_titles = dido_files_df.loc[1, :].values.tolist()\n",
    "dido_titles_df = pd.DataFrame(dido_titles, columns = ['titles'])\n",
    "\n",
    "# DELETE UNNEEDED ROWS\n",
    "dido_files_df = dido_files_df.iloc[2: , : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72fbce",
   "metadata": {},
   "source": [
    "### Import Above & Beyond Song Titles and Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "34eaef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relative path to folder containing the text files\n",
    "\n",
    "files_folder = \"/users/drzag/lyrics/abovebeyond\"\n",
    "files = []\n",
    "ab_files_df = pd.DataFrame()\n",
    "\n",
    "# Create a dataframe list and concatenate\n",
    "\n",
    "files = [pd.read_csv(file, delimiter='\\t') for file in glob.glob(os.path.join(files_folder ,\"*.txt\"))]\n",
    "ab_files_df = pd.concat(files, axis = 1)\n",
    "\n",
    "# EXTRACT TITLES ONLY\n",
    "ab_titles = []\n",
    "ab_titles = ab_files_df.loc[1, :].values.tolist()\n",
    "ab_titles_df = pd.DataFrame(ab_titles, columns = ['titles'])\n",
    "\n",
    "# DELETE UNNEEDED ROWS\n",
    "ab_files_df = ab_files_df.iloc[2: , : ]\n",
    "\n",
    "# DELETE DUPLICATE COLUMN\n",
    "ab_files_df = ab_files_df.loc[:,~ab_files_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47795e",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab79c94",
   "metadata": {},
   "source": [
    "## Functions to Preprocess Text Cells to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a3855d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_words(x):\n",
    "    x = re.sub('[^a-z\\s]', '', x.lower())\n",
    "    x = [w for w in x.split() if w not in set(sw)]\n",
    "    # USE THIS ONE FOR TOKENS\n",
    "    return x\n",
    "    # USE THIS ONE FOR STRING\n",
    "    #return ' '.join(x)\n",
    "\n",
    "def mod2_norm_tok(x):\n",
    "    '''\n",
    "    Now clean and tokenize your data. \n",
    "    Remove punctuation characters (available in the `punctuation` object in the `string` library)\n",
    "    Split on whitespace\n",
    "    Fold to lowercase\n",
    "    Remove stopwords.\n",
    "    '''\n",
    "    #Remove punctuation characters\n",
    "    x = x.translate(str.maketrans('', '', string.punctuation))\n",
    "    #Fold to lowercase\n",
    "    x = x.lower()\n",
    "    # starts with (feat.                #a few lyric began with a line for other featured artists\n",
    "    x = re.sub(r'^(feat).*[a-z]$', ' ', x)\n",
    "    # is nan\n",
    "    x = re.sub(r'(nan)', ' ', x)\n",
    "    #Split on whitespace AND Remove stopwords\n",
    "    x = [w for w in x.split() if w not in (set(sw))]\n",
    "    # USE THIS ONE FOR TOKENS\n",
    "    return x\n",
    "    # USE THIS ONE FOR STRING\n",
    "    #return ' '.join(x)\n",
    "    \n",
    "def leave_hashes(x):\n",
    "    '''\n",
    "    Now clean and tokenize your data. \n",
    "    Remove punctuation characters (available in the `punctuation` object in the `string` library)\n",
    "    Split on whitespace\n",
    "    Fold to lowercase\n",
    "    Remove stopwords.\n",
    "    '''\n",
    "    #Remove punctuation characters\n",
    "    x = x.translate(str.maketrans('', '', string.punctuation.replace(\"#\",\"\")))\n",
    "    #Fold to lowercase\n",
    "    x = x.lower()\n",
    "    # starts with (feat.                #a few lyric began with a line for other featured artists\n",
    "    x = re.sub(r'^(feat).*[a-z]$', ' ', x)\n",
    "    # is nan\n",
    "    x = re.sub(r'(nan)', ' ', x)\n",
    "    #Split on whitespace AND Remove stopwords\n",
    "    x = [w for w in x.split() if w not in (set(sw))]\n",
    "    # USE THIS ONE FOR TOKENS\n",
    "    return x\n",
    "    # USE THIS ONE FOR STRING\n",
    "    #return ' '.join(x)\n",
    "    \n",
    "def custom_clean(x):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    x = html.unescape(x) \n",
    "    # tags like <tab>\n",
    "    x = re.sub(r'<[^<>]*>', ' ', x)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    x = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', x)\n",
    "    # text or code in brackets like [0]\n",
    "    x = re.sub(r'\\[[^\\[\\]]*\\]', ' ', x)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    x = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', x)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    x = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', x)\n",
    "    # sequences of white spaces\n",
    "    x = re.sub(r'\\s+', ' ', x)\n",
    "    x = [w for w in x.split() if w not in set(sw)]      #Creates a LIST of tokens\n",
    "    #return x.strip()                                    #Removes spaces at beginning and end of string\n",
    "    return x                                            #RETURN for list of tokens\n",
    "    #return ' '.join(x)                                  #RETURN for single string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2dfca",
   "metadata": {},
   "source": [
    "## Insert Newly Processed Text into Dido DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "77098856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dido_fol['only_words'] = dido_fol['desc'].apply(only_words)\n",
    "dido_fol['assignment'] = dido_fol['desc'].apply(mod2_norm_tok)\n",
    "#dido_fol['custom'] = dido_fol['desc'].apply(custom_clean)\n",
    "dido_fol['with_hashes'] = dido_fol['desc'].apply(leave_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a167d97",
   "metadata": {},
   "source": [
    "## Insert Newly Processed Text into Above & Beyond DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cb32e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ab_fol['only_words'] = ab_fol['desc'].apply(only_words)\n",
    "ab_fol['assignment'] = ab_fol['desc'].apply(mod2_norm_tok)\n",
    "#ab_fol['custom'] = ab_fol['desc'].apply(custom_clean)\n",
    "ab_fol['with_hashes'] = ab_fol['desc'].apply(leave_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506c720",
   "metadata": {},
   "source": [
    "## Insert New Column for Processed Dido Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "947e3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "dido_titles_df['only_words'] = dido_titles_df['titles'].apply(only_words)\n",
    "dido_titles_df['assignment'] = dido_titles_df['titles'].apply(mod2_norm_tok)\n",
    "dido_titles_df['custom'] = dido_titles_df['titles'].apply(custom_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e551b3c",
   "metadata": {},
   "source": [
    "## Insert New Column for Processed Above & Beyond Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8ed95150",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_titles_df['only_words'] = ab_titles_df['titles'].apply(only_words)\n",
    "ab_titles_df['assignment'] = ab_titles_df['titles'].apply(mod2_norm_tok)\n",
    "ab_titles_df['custom'] = ab_titles_df['titles'].apply(custom_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2283d5",
   "metadata": {},
   "source": [
    "## Process Dido Lyrics into Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "509250ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All I See lyrics</th>\n",
       "      <th>All You Want lyrics</th>\n",
       "      <th>Blackbird lyrics</th>\n",
       "      <th>Burnin' Love lyrics</th>\n",
       "      <th>Cards lyrics</th>\n",
       "      <th>Chances lyrics</th>\n",
       "      <th>Closer lyrics</th>\n",
       "      <th>Clouds Like Islands lyrics</th>\n",
       "      <th>Day Before We Went To War lyrics</th>\n",
       "      <th>Don't Believe In Love lyrics</th>\n",
       "      <th>...</th>\n",
       "      <th>This Land Is Mine lyrics</th>\n",
       "      <th>Those Were The Days lyrics</th>\n",
       "      <th>Together lyrics</th>\n",
       "      <th>Us 2 Little Gods lyrics</th>\n",
       "      <th>Walking By lyrics</th>\n",
       "      <th>What Am I Doing Here lyrics</th>\n",
       "      <th>White Flag lyrics</th>\n",
       "      <th>Who Makes You Feel lyrics</th>\n",
       "      <th>Worthless lyrics</th>\n",
       "      <th>You Don't Need A God lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[id, like, watch, sleep, night]</td>\n",
       "      <td>[put, coffee, cup, milk, filled]</td>\n",
       "      <td>[found, peace]</td>\n",
       "      <td>[oh, baby, baby, baby]</td>\n",
       "      <td>[today]</td>\n",
       "      <td>[leave, taxi, waiting]</td>\n",
       "      <td>[pine, forests, clouds, like, islands]</td>\n",
       "      <td>[pictures, wall]</td>\n",
       "      <td>[wanna, go, bed]</td>\n",
       "      <td>...</td>\n",
       "      <td>[behind, walls, hear, song]</td>\n",
       "      <td>[kids]</td>\n",
       "      <td>[ladies, gentlemen]</td>\n",
       "      <td>[walking, home, burnt, red]</td>\n",
       "      <td>[keep, locked, keep, shut]</td>\n",
       "      <td>[theres, empty, seat, next, everywhere, go]</td>\n",
       "      <td>[know, think, shouldnt, still, love]</td>\n",
       "      <td>[dont, touch, way, used]</td>\n",
       "      <td>[know, came]</td>\n",
       "      <td>[hear, music]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[thought, done, packed, bags, walked, light]</td>\n",
       "      <td>[hear, breathe, side]</td>\n",
       "      <td>[added, sugar, never, knew, liked, like]</td>\n",
       "      <td>[lies, ive, told]</td>\n",
       "      <td>[happened, days]</td>\n",
       "      <td>[wake, watch, tv]</td>\n",
       "      <td>[turn, close, door]</td>\n",
       "      <td>[spanish, boys, mopeds]</td>\n",
       "      <td>[sand, lying, floor]</td>\n",
       "      <td>[arms, around]</td>\n",
       "      <td>...</td>\n",
       "      <td>[oh, sweet, words]</td>\n",
       "      <td>[wed, know]</td>\n",
       "      <td>[begun]</td>\n",
       "      <td>[sticky, thin, said]</td>\n",
       "      <td>[mind, still, roams, free]</td>\n",
       "      <td>[even, plane, sofa, back, home]</td>\n",
       "      <td>[tell]</td>\n",
       "      <td>[dont, call, write, im, away]</td>\n",
       "      <td>[know, youll, leave]</td>\n",
       "      <td>[notes, passing, car]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[picked, books, thought, closed, door, behind]</td>\n",
       "      <td>[though, sleep, leaves, behind]</td>\n",
       "      <td>[took, coat, hook, scarf, hat]</td>\n",
       "      <td>[im, hurt, blows, get, withheld]</td>\n",
       "      <td>[doors, always, open]</td>\n",
       "      <td>[another, wasted, day]</td>\n",
       "      <td>[sit, back, sitting]</td>\n",
       "      <td>[vodka, shots, charcoal, fires]</td>\n",
       "      <td>[bed, upstairs, still, unmade]</td>\n",
       "      <td>[wake]</td>\n",
       "      <td>...</td>\n",
       "      <td>[music, play, lights, world]</td>\n",
       "      <td>[live]</td>\n",
       "      <td>[dont, need, news]</td>\n",
       "      <td>[let, us, go, woods]</td>\n",
       "      <td>[keep, caged]</td>\n",
       "      <td>[lifes, short, time]</td>\n",
       "      <td>[didnt, say, well, id, still, felt]</td>\n",
       "      <td>[dont, make, love, often]</td>\n",
       "      <td>[came, heart]</td>\n",
       "      <td>[take]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[handed, back, keys, thought, free]</td>\n",
       "      <td>[theres, nowhere, id, rather]</td>\n",
       "      <td>[put, key, said, wont, needing]</td>\n",
       "      <td>[sharper, get, cut]</td>\n",
       "      <td>[music, always, played]</td>\n",
       "      <td>[thats, alright]</td>\n",
       "      <td>[little, closer]</td>\n",
       "      <td>[palm, trees, heat, asphalt]</td>\n",
       "      <td>[dust, float, light]</td>\n",
       "      <td>[pretend]</td>\n",
       "      <td>...</td>\n",
       "      <td>[sweetest, iâx80x99ve, heard]</td>\n",
       "      <td>[love, forever]</td>\n",
       "      <td>[need, weather]</td>\n",
       "      <td>[careful, said]</td>\n",
       "      <td>[tell, quiet]</td>\n",
       "      <td>[could, spent, two]</td>\n",
       "      <td>[wheres, sense]</td>\n",
       "      <td>[couldnt, wait, waits, usually, goes, away]</td>\n",
       "      <td>[lost]</td>\n",
       "      <td>[heart, opening, speed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[shame, dirt, surprises, wait]</td>\n",
       "      <td>[bed, oh, cold]</td>\n",
       "      <td>[turned, smiled, explained]</td>\n",
       "      <td>[harder, get, held]</td>\n",
       "      <td>[remember, darling]</td>\n",
       "      <td>[shadows, turn, clouds]</td>\n",
       "      <td>[look, serious]</td>\n",
       "      <td>[larks, lizards, wasps, grass]</td>\n",
       "      <td>[people, left]</td>\n",
       "      <td>[im, still, sleeping]</td>\n",
       "      <td>...</td>\n",
       "      <td>[could, iâx80x99ve, touched, turned]</td>\n",
       "      <td>[blind]</td>\n",
       "      <td>[tell, time]</td>\n",
       "      <td>[us, 2, little, gods, world, feet]</td>\n",
       "      <td>[remember, everything]</td>\n",
       "      <td>[ive, seen, ive, held]</td>\n",
       "      <td>[promise, im, trying, make, life, harder]</td>\n",
       "      <td>[listen, think, say]</td>\n",
       "      <td>[wont, stop]</td>\n",
       "      <td>[somewhere]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 All I See lyrics  \\\n",
       "2                                              []   \n",
       "3    [thought, done, packed, bags, walked, light]   \n",
       "4  [picked, books, thought, closed, door, behind]   \n",
       "5             [handed, back, keys, thought, free]   \n",
       "6                  [shame, dirt, surprises, wait]   \n",
       "\n",
       "               All You Want lyrics                          Blackbird lyrics  \\\n",
       "2  [id, like, watch, sleep, night]          [put, coffee, cup, milk, filled]   \n",
       "3            [hear, breathe, side]  [added, sugar, never, knew, liked, like]   \n",
       "4  [though, sleep, leaves, behind]            [took, coat, hook, scarf, hat]   \n",
       "5    [theres, nowhere, id, rather]           [put, key, said, wont, needing]   \n",
       "6                  [bed, oh, cold]               [turned, smiled, explained]   \n",
       "\n",
       "                Burnin' Love lyrics             Cards lyrics  \\\n",
       "2                    [found, peace]   [oh, baby, baby, baby]   \n",
       "3                 [lies, ive, told]         [happened, days]   \n",
       "4  [im, hurt, blows, get, withheld]    [doors, always, open]   \n",
       "5               [sharper, get, cut]  [music, always, played]   \n",
       "6               [harder, get, held]      [remember, darling]   \n",
       "\n",
       "            Chances lyrics           Closer lyrics  \\\n",
       "2                  [today]  [leave, taxi, waiting]   \n",
       "3        [wake, watch, tv]     [turn, close, door]   \n",
       "4   [another, wasted, day]    [sit, back, sitting]   \n",
       "5         [thats, alright]        [little, closer]   \n",
       "6  [shadows, turn, clouds]         [look, serious]   \n",
       "\n",
       "               Clouds Like Islands lyrics Day Before We Went To War lyrics  \\\n",
       "2  [pine, forests, clouds, like, islands]                 [pictures, wall]   \n",
       "3                 [spanish, boys, mopeds]             [sand, lying, floor]   \n",
       "4         [vodka, shots, charcoal, fires]   [bed, upstairs, still, unmade]   \n",
       "5            [palm, trees, heat, asphalt]             [dust, float, light]   \n",
       "6          [larks, lizards, wasps, grass]                   [people, left]   \n",
       "\n",
       "  Don't Believe In Love lyrics  ...              This Land Is Mine lyrics  \\\n",
       "2             [wanna, go, bed]  ...           [behind, walls, hear, song]   \n",
       "3               [arms, around]  ...                    [oh, sweet, words]   \n",
       "4                       [wake]  ...          [music, play, lights, world]   \n",
       "5                    [pretend]  ...         [sweetest, iâx80x99ve, heard]   \n",
       "6        [im, still, sleeping]  ...  [could, iâx80x99ve, touched, turned]   \n",
       "\n",
       "  Those Were The Days lyrics      Together lyrics  \\\n",
       "2                     [kids]  [ladies, gentlemen]   \n",
       "3                [wed, know]              [begun]   \n",
       "4                     [live]   [dont, need, news]   \n",
       "5            [love, forever]      [need, weather]   \n",
       "6                    [blind]         [tell, time]   \n",
       "\n",
       "              Us 2 Little Gods lyrics           Walking By lyrics  \\\n",
       "2         [walking, home, burnt, red]  [keep, locked, keep, shut]   \n",
       "3                [sticky, thin, said]  [mind, still, roams, free]   \n",
       "4                [let, us, go, woods]               [keep, caged]   \n",
       "5                     [careful, said]               [tell, quiet]   \n",
       "6  [us, 2, little, gods, world, feet]      [remember, everything]   \n",
       "\n",
       "                   What Am I Doing Here lyrics  \\\n",
       "2  [theres, empty, seat, next, everywhere, go]   \n",
       "3              [even, plane, sofa, back, home]   \n",
       "4                         [lifes, short, time]   \n",
       "5                          [could, spent, two]   \n",
       "6                       [ive, seen, ive, held]   \n",
       "\n",
       "                           White Flag lyrics  \\\n",
       "2       [know, think, shouldnt, still, love]   \n",
       "3                                     [tell]   \n",
       "4        [didnt, say, well, id, still, felt]   \n",
       "5                            [wheres, sense]   \n",
       "6  [promise, im, trying, make, life, harder]   \n",
       "\n",
       "                     Who Makes You Feel lyrics      Worthless lyrics  \\\n",
       "2                     [dont, touch, way, used]          [know, came]   \n",
       "3                [dont, call, write, im, away]  [know, youll, leave]   \n",
       "4                    [dont, make, love, often]         [came, heart]   \n",
       "5  [couldnt, wait, waits, usually, goes, away]                [lost]   \n",
       "6                         [listen, think, say]          [wont, stop]   \n",
       "\n",
       "  You Don't Need A God lyrics  \n",
       "2               [hear, music]  \n",
       "3       [notes, passing, car]  \n",
       "4                      [take]  \n",
       "5     [heart, opening, speed]  \n",
       "6                 [somewhere]  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in dido_files_df:\n",
    "    dido_files_df[column] = dido_files_df[column].apply(str)\n",
    "    dido_files_df[column] = dido_files_df[column].apply(mod2_norm_tok)\n",
    "dido_files_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e52a99",
   "metadata": {},
   "source": [
    "## Process Above & Beyond Lyrics into Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d475e420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Air For Life lyrics</th>\n",
       "      <th>Alchemy lyrics</th>\n",
       "      <th>All Over The World lyrics</th>\n",
       "      <th>Almost Home lyrics</th>\n",
       "      <th>Alone Tonight lyrics</th>\n",
       "      <th>Alright Now lyrics</th>\n",
       "      <th>Alright Now (Above &amp; Beyond Club Mix) lyrics</th>\n",
       "      <th>Always lyrics</th>\n",
       "      <th>Another Chance lyrics</th>\n",
       "      <th>Bittersweet &amp; Blue lyrics</th>\n",
       "      <th>...</th>\n",
       "      <th>Sticky Fingers (Acoustic Version) lyrics</th>\n",
       "      <th>Sun &amp; Moon lyrics</th>\n",
       "      <th>Sweetest Heart lyrics</th>\n",
       "      <th>Thing Called Love lyrics</th>\n",
       "      <th>Tightrope lyrics</th>\n",
       "      <th>Treasure lyrics</th>\n",
       "      <th>We're All We Need lyrics</th>\n",
       "      <th>With Your Hope lyrics</th>\n",
       "      <th>You Gotta Go lyrics</th>\n",
       "      <th>You Got To Go lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[need, air, lifewe, breath]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[slipping, sideways, silver, stars, collide]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[justine, suissa]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ask, another, chance]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[painted, picture, want]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[youre, running, tightrope]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[connected, talk]</td>\n",
       "      <td>[dream, little, dreamer]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[need, air, lifenow, soul]</td>\n",
       "      <td>[words, speak]</td>\n",
       "      <td>[maybe, waited, long]</td>\n",
       "      <td>[made]</td>\n",
       "      <td>[fade, away, like, love, died]</td>\n",
       "      <td>[ive, lived, little]</td>\n",
       "      <td>[ive, lived, little]</td>\n",
       "      <td>[maybe, hurting]</td>\n",
       "      <td>[ask]</td>\n",
       "      <td>[ive, got, photograph]</td>\n",
       "      <td>...</td>\n",
       "      <td>[wanted]</td>\n",
       "      <td>[raining, pouring]</td>\n",
       "      <td>[sweetest, heart]</td>\n",
       "      <td>[time, place]</td>\n",
       "      <td>[theres, empty, sky]</td>\n",
       "      <td>[treasure, measured, units, love]</td>\n",
       "      <td>[tell, place, like, go]</td>\n",
       "      <td>[singing, forest, ice]</td>\n",
       "      <td>[begins]</td>\n",
       "      <td>[dream, little, dreamer, begins]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[need, air, forever]</td>\n",
       "      <td>[foreign, land]</td>\n",
       "      <td>[maybe, got, caught, storm]</td>\n",
       "      <td>[ways, deceive]</td>\n",
       "      <td>[nowhere, universe, hide, tonight]</td>\n",
       "      <td>[ive, loved, lot]</td>\n",
       "      <td>[ive, loved, lot]</td>\n",
       "      <td>[maybe, give, little, time]</td>\n",
       "      <td>[ask, another, chance]</td>\n",
       "      <td>[kept, box, memories]</td>\n",
       "      <td>...</td>\n",
       "      <td>[theres, something]</td>\n",
       "      <td>[black, sky, falling]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[fear, inside]</td>\n",
       "      <td>[whered, go]</td>\n",
       "      <td>[means, may, find, rich]</td>\n",
       "      <td>[say, reach, seize, sorrow]</td>\n",
       "      <td>[slow, motion, moment, tumbling]</td>\n",
       "      <td>[dream, little, dreamer]</td>\n",
       "      <td>[move, feet, feel, sweet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[]</td>\n",
       "      <td>[youre, telling, birds, dont, fly]</td>\n",
       "      <td>[sent, incomplete]</td>\n",
       "      <td>[bind, soul]</td>\n",
       "      <td>[ive, wrestled, angels, life]</td>\n",
       "      <td>[lost]</td>\n",
       "      <td>[lost]</td>\n",
       "      <td>[day, really, need]</td>\n",
       "      <td>[ask]</td>\n",
       "      <td>[feel, breakable]</td>\n",
       "      <td>...</td>\n",
       "      <td>[would, like]</td>\n",
       "      <td>[cold, tonight]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[witty, line, save, face]</td>\n",
       "      <td>[please, anyone, know]</td>\n",
       "      <td>[beyond, wildest, dreams]</td>\n",
       "      <td>[always, mean, act, say]</td>\n",
       "      <td>[till, lost, inside, sound]</td>\n",
       "      <td>[begins]</td>\n",
       "      <td>[dream, little, dreamer, follow, signs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[need, air, whateverthat]</td>\n",
       "      <td>[love]</td>\n",
       "      <td>[lonely, call, haunting]</td>\n",
       "      <td>[waiting, found]</td>\n",
       "      <td>[always, halos, wings, keep, blind]</td>\n",
       "      <td>[tied, dreams]</td>\n",
       "      <td>[tied, dreams, lovers, knot]</td>\n",
       "      <td>[take, arms, say]</td>\n",
       "      <td>[know, time, come]</td>\n",
       "      <td>[take, look, eyes]</td>\n",
       "      <td>...</td>\n",
       "      <td>[get, sticky, fingers, head]</td>\n",
       "      <td>[gave, answer]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[parachute, pride]</td>\n",
       "      <td>[youre, running, tightrope]</td>\n",
       "      <td>[beyond, wildest, dreams]</td>\n",
       "      <td>[come, oh, darling, hurry, hurry, lets, go]</td>\n",
       "      <td>[worst, fall]</td>\n",
       "      <td>[move, feet]</td>\n",
       "      <td>[got, gather, need]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Air For Life lyrics                      Alchemy lyrics  \\\n",
       "2  [need, air, lifewe, breath]                                  []   \n",
       "3   [need, air, lifenow, soul]                      [words, speak]   \n",
       "4         [need, air, forever]                     [foreign, land]   \n",
       "5                           []  [youre, telling, birds, dont, fly]   \n",
       "6    [need, air, whateverthat]                              [love]   \n",
       "\n",
       "     All Over The World lyrics Almost Home lyrics  \\\n",
       "2                           []                 []   \n",
       "3        [maybe, waited, long]             [made]   \n",
       "4  [maybe, got, caught, storm]    [ways, deceive]   \n",
       "5           [sent, incomplete]       [bind, soul]   \n",
       "6     [lonely, call, haunting]   [waiting, found]   \n",
       "\n",
       "                           Alone Tonight lyrics    Alright Now lyrics  \\\n",
       "2  [slipping, sideways, silver, stars, collide]                    []   \n",
       "3                [fade, away, like, love, died]  [ive, lived, little]   \n",
       "4            [nowhere, universe, hide, tonight]     [ive, loved, lot]   \n",
       "5                 [ive, wrestled, angels, life]                [lost]   \n",
       "6           [always, halos, wings, keep, blind]        [tied, dreams]   \n",
       "\n",
       "  Alright Now (Above & Beyond Club Mix) lyrics                Always lyrics  \\\n",
       "2                            [justine, suissa]                           []   \n",
       "3                         [ive, lived, little]             [maybe, hurting]   \n",
       "4                            [ive, loved, lot]  [maybe, give, little, time]   \n",
       "5                                       [lost]          [day, really, need]   \n",
       "6                 [tied, dreams, lovers, knot]            [take, arms, say]   \n",
       "\n",
       "    Another Chance lyrics Bittersweet & Blue lyrics  ...  \\\n",
       "2  [ask, another, chance]                        []  ...   \n",
       "3                   [ask]    [ive, got, photograph]  ...   \n",
       "4  [ask, another, chance]     [kept, box, memories]  ...   \n",
       "5                   [ask]         [feel, breakable]  ...   \n",
       "6      [know, time, come]        [take, look, eyes]  ...   \n",
       "\n",
       "  Sticky Fingers (Acoustic Version) lyrics      Sun & Moon lyrics  \\\n",
       "2                 [painted, picture, want]                     []   \n",
       "3                                 [wanted]     [raining, pouring]   \n",
       "4                      [theres, something]  [black, sky, falling]   \n",
       "5                            [would, like]        [cold, tonight]   \n",
       "6             [get, sticky, fingers, head]         [gave, answer]   \n",
       "\n",
       "  Sweetest Heart lyrics   Thing Called Love lyrics  \\\n",
       "2                    []                         []   \n",
       "3     [sweetest, heart]              [time, place]   \n",
       "4                    []             [fear, inside]   \n",
       "5                    []  [witty, line, save, face]   \n",
       "6                    []         [parachute, pride]   \n",
       "\n",
       "              Tightrope lyrics                    Treasure lyrics  \\\n",
       "2  [youre, running, tightrope]                                 []   \n",
       "3         [theres, empty, sky]  [treasure, measured, units, love]   \n",
       "4                 [whered, go]           [means, may, find, rich]   \n",
       "5       [please, anyone, know]          [beyond, wildest, dreams]   \n",
       "6  [youre, running, tightrope]          [beyond, wildest, dreams]   \n",
       "\n",
       "                      We're All We Need lyrics  \\\n",
       "2                                           []   \n",
       "3                      [tell, place, like, go]   \n",
       "4                  [say, reach, seize, sorrow]   \n",
       "5                     [always, mean, act, say]   \n",
       "6  [come, oh, darling, hurry, hurry, lets, go]   \n",
       "\n",
       "              With Your Hope lyrics       You Gotta Go lyrics  \\\n",
       "2                 [connected, talk]  [dream, little, dreamer]   \n",
       "3            [singing, forest, ice]                  [begins]   \n",
       "4  [slow, motion, moment, tumbling]  [dream, little, dreamer]   \n",
       "5       [till, lost, inside, sound]                  [begins]   \n",
       "6                     [worst, fall]              [move, feet]   \n",
       "\n",
       "                      You Got To Go lyrics  \n",
       "2                                       []  \n",
       "3         [dream, little, dreamer, begins]  \n",
       "4                [move, feet, feel, sweet]  \n",
       "5  [dream, little, dreamer, follow, signs]  \n",
       "6                      [got, gather, need]  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in ab_files_df:\n",
    "    ab_files_df[column] = ab_files_df[column].apply(str)\n",
    "    ab_files_df[column] = ab_files_df[column].apply(mod2_norm_tok)\n",
    "ab_files_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d276ac3",
   "metadata": {},
   "source": [
    "#### Attempt to Load Tokens into One Large List: Too Slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49513472",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Loading one giant list takes WAYYYYY to long\n",
    "#text = []\n",
    "#for i in range(len(ab_fol)):\n",
    "#    text = text + ab_fol['clean_desc_tok'][i]\n",
    "#    print(i)\n",
    "#text\n",
    "\n",
    "#len(text)\n",
    "#counter = Counter(text)\n",
    "#print(counter)\n",
    "\n",
    "#freq_df = pd.DataFrame.from_dict(counter, orient = 'index', columns = ['freq'])\n",
    "#freq_df.index.name = 'token'\n",
    "#freq_df = freq_df.sort_values('freq', ascending = False)\n",
    "#freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b98fc",
   "metadata": {},
   "source": [
    "## Function: Convert List of Tokens to Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "475f93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "###########################################\n",
    "#If not done, load text Token Lists into DataFrame Cells: One Token List per Cell:\n",
    "#Example for one line of text:\n",
    "#text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "#text_df = pd.DataFrame(data={'desc': [text]})\n",
    "\n",
    "def descriptive_stats(df_col, top_x_tokens = 10, verbose=True) :\n",
    "    counter = Counter()\n",
    "    df_col.map(counter.update)\n",
    "    num_tokens = sum(counter.values())\n",
    "    num_unique_tokens = len(counter.keys())\n",
    "    num_characters=0\n",
    "    for key, value in counter.items():\n",
    "        char = (len(key))*value\n",
    "        num_characters = num_characters + char\n",
    "    lexical_diversity = num_unique_tokens/num_tokens\n",
    "\n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "        print(f\"The ten most common tokens are:\")\n",
    "        print(counter.most_common(top_x_tokens))\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "\n",
    "def descriptive_stats2(df_col, top_x_tokens = 10, verbose=True) :\n",
    "    song_counter = Counter()\n",
    "    df_col.map(song_counter.update)\n",
    "    song_length.append(sum(song_counter.values()))\n",
    "    \n",
    "    df_col.map(counter.update)\n",
    "    num_tokens = sum(counter.values())\n",
    "    num_unique_tokens = len(counter.keys())\n",
    "    num_characters=0\n",
    "    for key, value in counter.items():\n",
    "        char = (len(key))*value\n",
    "        num_characters = num_characters + char\n",
    "    lexical_diversity = num_unique_tokens/num_tokens\n",
    "\n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "        print(f\"The ten most common tokens are:\")\n",
    "        common = counter.most_common(top_x_tokens)\n",
    "        print(common)\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d08eb",
   "metadata": {},
   "source": [
    "# Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0779295",
   "metadata": {},
   "source": [
    "### Dido Followers -- Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e0ef25bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 509393 tokens in the data.\n",
      "There are 114085 unique tokens in the data.\n",
      "There are 3001470 characters in the data.\n",
      "The lexical diversity is 0.224 in the data.\n",
      "The ten most common tokens are:\n",
      "[('de', 7903), ('love', 4649), ('la', 4499), ('music', 4290), ('en', 3153), ('life', 2992), ('im', 2774), ('que', 2574), ('el', 2148), ('fan', 1970), ('lover', 1614), ('follow', 1343), ('mi', 1324), ('like', 1308), ('live', 1280), ('e', 1194), ('un', 1183), ('es', 1157), ('los', 1149), ('world', 1139)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[509393, 114085, 0.22396263788469806, 3001470]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stats(dido_fol['assignment'], top_x_tokens = 20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588dcdc",
   "metadata": {},
   "source": [
    "### Above & Beyond Followers -- Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "23c2d5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 466536 tokens in the data.\n",
      "There are 112024 unique tokens in the data.\n",
      "There are 2796392 characters in the data.\n",
      "The lexical diversity is 0.240 in the data.\n",
      "The ten most common tokens are:\n",
      "[('music', 7029), ('de', 4062), ('dj', 3416), ('love', 2936), ('la', 2812), ('life', 2577), ('producer', 2543), ('trance', 2028), ('•', 1970), ('im', 1672), ('house', 1567), ('en', 1558), ('edm', 1461), ('que', 1423), ('lover', 1305), ('like', 1274), ('el', 1241), ('fan', 1171), ('música', 1108), ('live', 1060)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[466536, 112024, 0.24011866179673166, 2796392]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stats(ab_fol['assignment'], top_x_tokens = 20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58ed0b",
   "metadata": {},
   "source": [
    "## Convert List of Tokens to Stats: Text Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2f90e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "The ten most common tokens are:\n",
      "[('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1), ('with', 1), ('other', 1), ('in', 1), ('this', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13, 9, 0.6923076923076923, 55]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "text_df = pd.DataFrame(data={'desc': [text]})\n",
    "\n",
    "descriptive_stats(text_df['desc'], top_x_tokens = 20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "59dcf058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "The ten most common tokens are:\n",
      "[('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1), ('with', 1), ('other', 1), ('in', 1), ('this', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text_df['desc'], verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text_df['desc'], verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text_df['desc'], verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text_df['desc'], verbose=False)[3] == 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: The assert statement will throw an error is the assertion, in this case a comparative statement, is not True.  Since there were no errors thrown, the function is returning the expected values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a488746",
   "metadata": {},
   "source": [
    "## Dido Lyrics -- Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "00b6f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 198 tokens in the data.\n",
      "There are 145 unique tokens in the data.\n",
      "There are 1003 characters in the data.\n",
      "The lexical diversity is 0.732 in the data.\n",
      "The ten most common tokens are:\n",
      "[('youre', 8), ('see', 8), ('carry', 6), ('thought', 5), ('back', 5), ('need', 4), ('theyre', 4), ('would', 3), ('im', 3), ('could', 3), ('find', 3), ('without', 3), ('dont', 3), ('gone', 2), ('memories', 2), ('still', 2), ('fly', 2), ('right', 2), ('change', 2), ('wrong', 2)]\n",
      "There are 294 tokens in the data.\n",
      "There are 184 unique tokens in the data.\n",
      "There are 1435 characters in the data.\n",
      "The lexical diversity is 0.626 in the data.\n",
      "The ten most common tokens are:\n",
      "[('want', 8), ('youre', 8), ('see', 8), ('need', 7), ('carry', 6), ('thought', 5), ('back', 5), ('right', 5), ('sleep', 5), ('room', 4), ('id', 4), ('would', 4), ('could', 4), ('theyre', 4), ('night', 4), ('gone', 3), ('im', 3), ('like', 3), ('find', 3), ('without', 3)]\n",
      "There are 398 tokens in the data.\n",
      "There are 240 unique tokens in the data.\n",
      "There are 1936 characters in the data.\n",
      "The lexical diversity is 0.603 in the data.\n",
      "The ten most common tokens are:\n",
      "[('back', 8), ('want', 8), ('youre', 8), ('see', 8), ('need', 7), ('heart', 7), ('carry', 6), ('thought', 5), ('right', 5), ('hands', 5), ('sleep', 5), ('watch', 5), ('room', 4), ('id', 4), ('would', 4), ('like', 4), ('could', 4), ('away', 4), ('without', 4), ('theyre', 4)]\n",
      "There are 500 tokens in the data.\n",
      "There are 253 unique tokens in the data.\n",
      "There are 2419 characters in the data.\n",
      "The lexical diversity is 0.506 in the data.\n",
      "The ten most common tokens are:\n",
      "[('ive', 17), ('youre', 15), ('without', 10), ('love', 10), ('whaoao', 10), ('one', 9), ('back', 8), ('want', 8), ('see', 8), ('need', 7), ('heart', 7), ('burnin', 7), ('calling', 7), ('carry', 6), ('get', 6), ('thought', 5), ('im', 5), ('right', 5), ('hands', 5), ('sleep', 5)]\n",
      "There are 578 tokens in the data.\n",
      "There are 274 unique tokens in the data.\n",
      "There are 2787 characters in the data.\n",
      "The lexical diversity is 0.474 in the data.\n",
      "The ten most common tokens are:\n",
      "[('ive', 17), ('loved', 17), ('youre', 15), ('without', 10), ('love', 10), ('whaoao', 10), ('see', 9), ('still', 9), ('one', 9), ('back', 8), ('want', 8), ('way', 7), ('need', 7), ('heart', 7), ('burnin', 7), ('calling', 7), ('carry', 6), ('never', 6), ('get', 6), ('baby', 6)]\n",
      "There are 657 tokens in the data.\n",
      "There are 298 unique tokens in the data.\n",
      "There are 3179 characters in the data.\n",
      "The lexical diversity is 0.454 in the data.\n",
      "The ten most common tokens are:\n",
      "[('ive', 17), ('loved', 17), ('youre', 15), ('see', 12), ('want', 11), ('without', 10), ('love', 10), ('whaoao', 10), ('still', 9), ('one', 9), ('back', 8), ('wont', 8), ('way', 7), ('need', 7), ('heart', 7), ('burnin', 7), ('calling', 7), ('carry', 6), ('right', 6), ('watch', 6)]\n",
      "There are 787 tokens in the data.\n",
      "There are 328 unique tokens in the data.\n",
      "There are 3839 characters in the data.\n",
      "The lexical diversity is 0.417 in the data.\n",
      "The ten most common tokens are:\n",
      "[('youre', 22), ('see', 19), ('ive', 19), ('loved', 17), ('get', 14), ('closer', 14), ('want', 12), ('better', 11), ('still', 10), ('without', 10), ('feel', 10), ('love', 10), ('whaoao', 10), ('back', 9), ('one', 9), ('need', 8), ('wont', 8), ('way', 7), ('look', 7), ('heart', 7)]\n",
      "There are 870 tokens in the data.\n",
      "There are 389 unique tokens in the data.\n",
      "There are 4306 characters in the data.\n",
      "The lexical diversity is 0.447 in the data.\n",
      "The ten most common tokens are:\n",
      "[('youre', 22), ('see', 19), ('ive', 19), ('loved', 17), ('get', 14), ('closer', 14), ('want', 12), ('one', 11), ('better', 11), ('still', 10), ('without', 10), ('feel', 10), ('love', 10), ('whaoao', 10), ('back', 9), ('need', 8), ('wont', 8), ('like', 7), ('way', 7), ('look', 7)]\n",
      "There are 991 tokens in the data.\n",
      "There are 434 unique tokens in the data.\n",
      "There are 4873 characters in the data.\n",
      "The lexical diversity is 0.438 in the data.\n",
      "The ten most common tokens are:\n",
      "[('youre', 22), ('see', 19), ('ive', 19), ('feel', 18), ('loved', 17), ('back', 16), ('get', 14), ('closer', 14), ('still', 13), ('like', 13), ('want', 12), ('one', 12), ('day', 12), ('better', 11), ('without', 10), ('love', 10), ('whaoao', 10), ('light', 8), ('need', 8), ('wont', 8)]\n",
      "There are 1113 tokens in the data.\n",
      "There are 462 unique tokens in the data.\n",
      "There are 5495 characters in the data.\n",
      "The lexical diversity is 0.415 in the data.\n",
      "The ten most common tokens are:\n",
      "[('youre', 25), ('love', 23), ('see', 19), ('ive', 19), ('feel', 19), ('loved', 17), ('back', 16), ('believe', 16), ('dont', 16), ('still', 14), ('get', 14), ('closer', 14), ('like', 13), ('want', 12), ('one', 12), ('day', 12), ('better', 11), ('without', 10), ('whaoao', 10), ('nothing', 9)]\n",
      "There are 1234 tokens in the data.\n",
      "There are 482 unique tokens in the data.\n",
      "There are 6027 characters in the data.\n",
      "The lexical diversity is 0.391 in the data.\n",
      "The ten most common tokens are:\n",
      "[('youre', 31), ('dont', 27), ('love', 24), ('ive', 21), ('see', 19), ('feel', 19), ('loved', 17), ('back', 16), ('believe', 16), ('oh', 16), ('like', 15), ('leave', 15), ('still', 14), ('get', 14), ('closer', 14), ('one', 13), ('day', 13), ('want', 12), ('need', 12), ('home', 12)]\n",
      "There are 1338 tokens in the data.\n",
      "There are 510 unique tokens in the data.\n",
      "There are 6498 characters in the data.\n",
      "The lexical diversity is 0.381 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 37), ('youre', 34), ('love', 24), ('see', 22), ('ive', 21), ('oh', 21), ('feel', 20), ('loved', 17), ('back', 16), ('believe', 16), ('like', 15), ('leave', 15), ('still', 14), ('one', 14), ('get', 14), ('closer', 14), ('day', 13), ('want', 12), ('need', 12), ('home', 12)]\n",
      "There are 1462 tokens in the data.\n",
      "There are 529 unique tokens in the data.\n",
      "There are 7109 characters in the data.\n",
      "The lexical diversity is 0.362 in the data.\n",
      "The ten most common tokens are:\n",
      "[('youre', 39), ('dont', 37), ('see', 25), ('love', 25), ('time', 22), ('ive', 21), ('oh', 21), ('feel', 21), ('still', 19), ('like', 19), ('loved', 17), ('little', 17), ('back', 16), ('believe', 16), ('leave', 15), ('one', 14), ('get', 14), ('closer', 14), ('day', 13), ('want', 12)]\n",
      "There are 1595 tokens in the data.\n",
      "There are 557 unique tokens in the data.\n",
      "There are 7724 characters in the data.\n",
      "The lexical diversity is 0.349 in the data.\n",
      "The ten most common tokens are:\n",
      "[('youre', 40), ('dont', 37), ('see', 35), ('love', 30), ('feel', 24), ('time', 23), ('ive', 21), ('oh', 21), ('still', 19), ('like', 19), ('loved', 17), ('little', 17), ('back', 16), ('believe', 16), ('one', 16), ('need', 15), ('leave', 15), ('nothing', 15), ('day', 14), ('get', 14)]\n",
      "There are 1706 tokens in the data.\n",
      "There are 592 unique tokens in the data.\n",
      "There are 8395 characters in the data.\n",
      "The lexical diversity is 0.347 in the data.\n",
      "The ten most common tokens are:\n",
      "[('youre', 40), ('see', 37), ('dont', 37), ('love', 34), ('ive', 29), ('feel', 24), ('time', 23), ('oh', 21), ('still', 19), ('like', 19), ('one', 17), ('loved', 17), ('little', 17), ('back', 16), ('believe', 16), ('need', 16), ('nothing', 16), ('leave', 15), ('day', 14), ('get', 14)]\n",
      "There are 1797 tokens in the data.\n",
      "There are 592 unique tokens in the data.\n",
      "There are 8972 characters in the data.\n",
      "The lexical diversity is 0.329 in the data.\n",
      "The ten most common tokens are:\n",
      "[('youre', 40), ('see', 39), ('love', 38), ('dont', 37), ('ive', 33), ('feel', 24), ('time', 23), ('oh', 21), ('still', 19), ('like', 19), ('one', 18), ('need', 17), ('loved', 17), ('little', 17), ('nothing', 17), ('back', 16), ('believe', 16), ('leave', 15), ('away', 14), ('home', 14)]\n",
      "There are 1969 tokens in the data.\n",
      "There are 622 unique tokens in the data.\n",
      "There are 9755 characters in the data.\n",
      "The lexical diversity is 0.316 in the data.\n",
      "The ten most common tokens are:\n",
      "[('see', 46), ('ive', 45), ('youre', 40), ('love', 38), ('dont', 37), ('time', 29), ('feel', 24), ('oh', 23), ('one', 20), ('still', 19), ('like', 19), ('back', 17), ('need', 17), ('loved', 17), ('little', 17), ('nothing', 17), ('believe', 16), ('could', 16), ('leave', 15), ('day', 15)]\n",
      "There are 2092 tokens in the data.\n",
      "There are 645 unique tokens in the data.\n",
      "There are 10253 characters in the data.\n",
      "The lexical diversity is 0.308 in the data.\n",
      "The ten most common tokens are:\n",
      "[('see', 49), ('ive', 45), ('youre', 40), ('love', 38), ('dont', 37), ('time', 29), ('one', 29), ('day', 25), ('feel', 24), ('oh', 23), ('back', 19), ('still', 19), ('like', 19), ('id', 18), ('need', 18), ('could', 17), ('loved', 17), ('little', 17), ('nothing', 17), ('believe', 16)]\n",
      "There are 2220 tokens in the data.\n",
      "There are 657 unique tokens in the data.\n",
      "There are 10870 characters in the data.\n",
      "The lexical diversity is 0.296 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 55), ('see', 51), ('ive', 47), ('youre', 40), ('love', 38), ('feel', 30), ('time', 29), ('one', 29), ('day', 25), ('oh', 23), ('back', 20), ('id', 19), ('still', 19), ('like', 19), ('need', 18), ('could', 17), ('away', 17), ('loved', 17), ('little', 17), ('nothing', 17)]\n",
      "There are 2331 tokens in the data.\n",
      "There are 682 unique tokens in the data.\n",
      "There are 11390 characters in the data.\n",
      "The lexical diversity is 0.293 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 59), ('see', 51), ('ive', 47), ('youre', 40), ('love', 38), ('feel', 30), ('time', 29), ('one', 29), ('day', 27), ('oh', 23), ('like', 22), ('back', 21), ('away', 20), ('loved', 20), ('id', 19), ('still', 19), ('need', 18), ('get', 18), ('nothing', 18), ('could', 17)]\n",
      "There are 2485 tokens in the data.\n",
      "There are 697 unique tokens in the data.\n",
      "There are 12059 characters in the data.\n",
      "The lexical diversity is 0.280 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 67), ('see', 51), ('ive', 47), ('youre', 40), ('love', 38), ('feel', 30), ('time', 29), ('one', 29), ('oh', 27), ('day', 27), ('like', 22), ('go', 22), ('back', 21), ('nothing', 21), ('away', 20), ('get', 20), ('loved', 20), ('id', 19), ('still', 19), ('leave', 19)]\n",
      "There are 2599 tokens in the data.\n",
      "There are 719 unique tokens in the data.\n",
      "There are 12625 characters in the data.\n",
      "The lexical diversity is 0.277 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 67), ('see', 51), ('ive', 47), ('youre', 40), ('love', 38), ('go', 30), ('feel', 30), ('time', 29), ('oh', 29), ('one', 29), ('day', 27), ('id', 25), ('still', 25), ('like', 22), ('back', 21), ('nothing', 21), ('believe', 20), ('away', 20), ('get', 20), ('loved', 20)]\n",
      "There are 2691 tokens in the data.\n",
      "There are 735 unique tokens in the data.\n",
      "There are 13104 characters in the data.\n",
      "The lexical diversity is 0.273 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 68), ('see', 52), ('ive', 47), ('youre', 41), ('love', 40), ('time', 33), ('go', 31), ('feel', 30), ('oh', 29), ('one', 29), ('day', 27), ('still', 26), ('id', 25), ('like', 23), ('nothing', 23), ('back', 21), ('leave', 21), ('believe', 20), ('away', 20), ('get', 20)]\n",
      "There are 2823 tokens in the data.\n",
      "There are 758 unique tokens in the data.\n",
      "There are 13713 characters in the data.\n",
      "The lexical diversity is 0.269 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 68), ('see', 53), ('ive', 47), ('youre', 41), ('love', 40), ('time', 33), ('go', 32), ('ill', 31), ('feel', 30), ('oh', 29), ('one', 29), ('leave', 28), ('day', 28), ('still', 26), ('id', 25), ('like', 23), ('nothing', 23), ('back', 21), ('believe', 20), ('away', 20)]\n",
      "There are 2921 tokens in the data.\n",
      "There are 768 unique tokens in the data.\n",
      "There are 14154 characters in the data.\n",
      "The lexical diversity is 0.263 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 71), ('see', 53), ('ive', 47), ('youre', 45), ('love', 42), ('ill', 35), ('time', 33), ('go', 32), ('leave', 31), ('feel', 30), ('oh', 29), ('one', 29), ('day', 29), ('still', 27), ('id', 25), ('away', 24), ('nothing', 24), ('like', 23), ('back', 21), ('know', 21)]\n",
      "There are 3017 tokens in the data.\n",
      "There are 779 unique tokens in the data.\n",
      "There are 14551 characters in the data.\n",
      "The lexical diversity is 0.258 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 71), ('see', 53), ('ive', 48), ('youre', 47), ('love', 43), ('ill', 35), ('time', 35), ('im', 34), ('go', 33), ('leave', 31), ('feel', 30), ('oh', 29), ('one', 29), ('day', 29), ('still', 27), ('id', 25), ('like', 24), ('away', 24), ('nothing', 24), ('back', 22)]\n",
      "There are 3064 tokens in the data.\n",
      "There are 786 unique tokens in the data.\n",
      "There are 14776 characters in the data.\n",
      "The lexical diversity is 0.257 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 73), ('see', 53), ('youre', 49), ('ive', 48), ('love', 43), ('ill', 36), ('time', 35), ('im', 34), ('leave', 34), ('go', 34), ('oh', 30), ('feel', 30), ('one', 29), ('day', 29), ('still', 28), ('id', 25), ('like', 24), ('away', 24), ('wont', 24), ('nothing', 24)]\n",
      "There are 3104 tokens in the data.\n",
      "There are 787 unique tokens in the data.\n",
      "There are 14957 characters in the data.\n",
      "The lexical diversity is 0.254 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 75), ('see', 53), ('youre', 49), ('ive', 48), ('love', 43), ('im', 37), ('ill', 36), ('time', 35), ('leave', 34), ('go', 34), ('feel', 34), ('oh', 30), ('day', 30), ('one', 29), ('still', 28), ('want', 26), ('id', 25), ('like', 24), ('away', 24), ('wont', 24)]\n",
      "There are 3204 tokens in the data.\n",
      "There are 803 unique tokens in the data.\n",
      "There are 15394 characters in the data.\n",
      "The lexical diversity is 0.251 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 76), ('see', 56), ('youre', 50), ('ive', 48), ('love', 43), ('go', 39), ('im', 38), ('ill', 37), ('time', 36), ('leave', 35), ('feel', 34), ('one', 33), ('oh', 31), ('day', 30), ('want', 28), ('still', 28), ('id', 25), ('like', 24), ('away', 24), ('wont', 24)]\n",
      "There are 3287 tokens in the data.\n",
      "There are 809 unique tokens in the data.\n",
      "There are 15818 characters in the data.\n",
      "The lexical diversity is 0.246 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 76), ('see', 57), ('youre', 50), ('ive', 48), ('love', 43), ('go', 39), ('im', 38), ('ill', 37), ('time', 37), ('leave', 35), ('feel', 34), ('one', 33), ('oh', 31), ('let', 30), ('day', 30), ('want', 28), ('still', 28), ('away', 26), ('id', 25), ('nothing', 25)]\n",
      "There are 3376 tokens in the data.\n",
      "There are 822 unique tokens in the data.\n",
      "There are 16216 characters in the data.\n",
      "The lexical diversity is 0.243 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 76), ('see', 57), ('ive', 53), ('youre', 52), ('love', 43), ('go', 41), ('im', 38), ('ill', 37), ('time', 37), ('leave', 35), ('feel', 34), ('one', 34), ('oh', 31), ('let', 30), ('day', 30), ('want', 28), ('still', 28), ('id', 26), ('away', 26), ('nothing', 25)]\n",
      "There are 3490 tokens in the data.\n",
      "There are 831 unique tokens in the data.\n",
      "There are 16697 characters in the data.\n",
      "The lexical diversity is 0.238 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 80), ('see', 58), ('ive', 53), ('youre', 52), ('im', 52), ('love', 43), ('go', 41), ('ill', 39), ('time', 38), ('leave', 35), ('feel', 34), ('one', 34), ('oh', 31), ('let', 30), ('day', 30), ('want', 28), ('still', 28), ('wont', 28), ('know', 28), ('id', 27)]\n",
      "There are 3580 tokens in the data.\n",
      "There are 844 unique tokens in the data.\n",
      "There are 17120 characters in the data.\n",
      "The lexical diversity is 0.236 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 81), ('see', 58), ('youre', 56), ('im', 54), ('ive', 53), ('love', 43), ('go', 41), ('ill', 39), ('time', 38), ('leave', 36), ('feel', 34), ('one', 34), ('day', 34), ('oh', 33), ('let', 31), ('want', 28), ('still', 28), ('wont', 28), ('know', 28), ('id', 27)]\n",
      "There are 3693 tokens in the data.\n",
      "There are 858 unique tokens in the data.\n",
      "There are 17668 characters in the data.\n",
      "The lexical diversity is 0.232 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 85), ('see', 58), ('youre', 56), ('im', 54), ('ive', 53), ('love', 43), ('go', 41), ('ill', 39), ('time', 38), ('one', 38), ('feel', 37), ('leave', 36), ('day', 34), ('oh', 33), ('let', 31), ('wont', 29), ('home', 29), ('know', 29), ('want', 28), ('still', 28)]\n",
      "There are 3781 tokens in the data.\n",
      "There are 869 unique tokens in the data.\n",
      "There are 18071 characters in the data.\n",
      "The lexical diversity is 0.230 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 89), ('youre', 60), ('see', 59), ('im', 54), ('ive', 53), ('love', 43), ('go', 41), ('ill', 39), ('time', 38), ('one', 38), ('feel', 37), ('leave', 36), ('day', 34), ('oh', 33), ('want', 32), ('know', 32), ('let', 31), ('wont', 30), ('home', 29), ('still', 28)]\n",
      "There are 3884 tokens in the data.\n",
      "There are 881 unique tokens in the data.\n",
      "There are 18526 characters in the data.\n",
      "The lexical diversity is 0.227 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 94), ('youre', 60), ('see', 59), ('im', 54), ('ive', 53), ('love', 43), ('go', 41), ('ill', 39), ('time', 39), ('one', 38), ('feel', 37), ('leave', 36), ('nothing', 35), ('day', 34), ('know', 33), ('oh', 33), ('want', 32), ('home', 32), ('away', 31), ('let', 31)]\n",
      "There are 4040 tokens in the data.\n",
      "There are 908 unique tokens in the data.\n",
      "There are 19222 characters in the data.\n",
      "The lexical diversity is 0.225 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 103), ('see', 62), ('youre', 60), ('im', 56), ('ive', 53), ('ill', 44), ('love', 44), ('go', 42), ('time', 40), ('one', 38), ('say', 37), ('feel', 37), ('leave', 36), ('day', 36), ('away', 35), ('know', 35), ('nothing', 35), ('want', 33), ('oh', 33), ('home', 32)]\n",
      "There are 4132 tokens in the data.\n",
      "There are 915 unique tokens in the data.\n",
      "There are 19650 characters in the data.\n",
      "The lexical diversity is 0.221 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 109), ('see', 62), ('youre', 60), ('im', 56), ('ive', 53), ('go', 46), ('ill', 45), ('love', 44), ('feel', 43), ('time', 40), ('away', 38), ('wont', 38), ('one', 38), ('day', 38), ('say', 37), ('leave', 36), ('know', 36), ('nothing', 36), ('want', 33), ('oh', 33)]\n",
      "There are 4342 tokens in the data.\n",
      "There are 958 unique tokens in the data.\n",
      "There are 20563 characters in the data.\n",
      "The lexical diversity is 0.221 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 111), ('see', 65), ('youre', 61), ('im', 59), ('let', 57), ('ive', 53), ('go', 48), ('love', 47), ('ill', 45), ('feel', 44), ('say', 40), ('time', 40), ('away', 39), ('wont', 38), ('one', 38), ('day', 38), ('leave', 36), ('know', 36), ('nothing', 36), ('want', 33)]\n",
      "There are 4480 tokens in the data.\n",
      "There are 973 unique tokens in the data.\n",
      "There are 21241 characters in the data.\n",
      "The lexical diversity is 0.217 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 116), ('see', 65), ('youre', 61), ('im', 61), ('let', 58), ('ive', 54), ('nothing', 50), ('love', 49), ('go', 48), ('ill', 45), ('feel', 44), ('say', 41), ('time', 40), ('wont', 40), ('away', 39), ('one', 38), ('day', 38), ('leave', 36), ('know', 36), ('want', 33)]\n",
      "There are 4569 tokens in the data.\n",
      "There are 991 unique tokens in the data.\n",
      "There are 21694 characters in the data.\n",
      "The lexical diversity is 0.217 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 116), ('see', 65), ('youre', 61), ('im', 61), ('let', 60), ('ive', 56), ('go', 50), ('nothing', 50), ('love', 49), ('ill', 47), ('feel', 44), ('say', 41), ('wont', 41), ('time', 40), ('away', 39), ('one', 39), ('day', 38), ('leave', 36), ('know', 36), ('home', 35)]\n",
      "There are 4602 tokens in the data.\n",
      "There are 992 unique tokens in the data.\n",
      "There are 21835 characters in the data.\n",
      "The lexical diversity is 0.216 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 120), ('see', 66), ('youre', 61), ('im', 61), ('let', 60), ('ive', 56), ('go', 50), ('nothing', 50), ('love', 49), ('ill', 47), ('feel', 44), ('say', 41), ('wont', 41), ('time', 40), ('away', 39), ('one', 39), ('day', 38), ('leave', 36), ('know', 36), ('home', 35)]\n",
      "There are 4705 tokens in the data.\n",
      "There are 1005 unique tokens in the data.\n",
      "There are 22324 characters in the data.\n",
      "The lexical diversity is 0.214 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 120), ('see', 67), ('im', 63), ('youre', 61), ('ive', 61), ('let', 60), ('nothing', 51), ('go', 50), ('love', 49), ('ill', 48), ('feel', 44), ('away', 41), ('say', 41), ('time', 41), ('wont', 41), ('home', 39), ('one', 39), ('day', 38), ('leave', 36), ('know', 36)]\n",
      "There are 4760 tokens in the data.\n",
      "There are 1012 unique tokens in the data.\n",
      "There are 22586 characters in the data.\n",
      "The lexical diversity is 0.213 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 120), ('see', 67), ('im', 63), ('youre', 61), ('ive', 61), ('let', 60), ('nothing', 51), ('go', 50), ('ill', 49), ('love', 49), ('feel', 44), ('say', 42), ('away', 41), ('time', 41), ('wont', 41), ('one', 40), ('home', 39), ('leave', 38), ('day', 38), ('know', 36)]\n",
      "There are 4862 tokens in the data.\n",
      "There are 1029 unique tokens in the data.\n",
      "There are 23072 characters in the data.\n",
      "The lexical diversity is 0.212 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 120), ('see', 67), ('im', 65), ('ive', 63), ('youre', 61), ('let', 60), ('love', 58), ('nothing', 57), ('go', 51), ('ill', 50), ('say', 44), ('feel', 44), ('time', 43), ('away', 41), ('wont', 41), ('one', 41), ('leave', 39), ('home', 39), ('day', 38), ('know', 36)]\n",
      "There are 4981 tokens in the data.\n",
      "There are 1042 unique tokens in the data.\n",
      "There are 23589 characters in the data.\n",
      "The lexical diversity is 0.209 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 120), ('love', 70), ('see', 67), ('im', 65), ('ive', 63), ('let', 63), ('youre', 61), ('nothing', 57), ('go', 51), ('ill', 50), ('say', 45), ('feel', 44), ('time', 43), ('away', 41), ('wont', 41), ('one', 41), ('day', 40), ('leave', 39), ('home', 39), ('want', 38)]\n",
      "There are 5085 tokens in the data.\n",
      "There are 1060 unique tokens in the data.\n",
      "There are 24083 characters in the data.\n",
      "The lexical diversity is 0.208 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 121), ('see', 72), ('love', 70), ('im', 66), ('ive', 64), ('let', 63), ('youre', 61), ('nothing', 57), ('ill', 52), ('go', 51), ('say', 45), ('feel', 44), ('time', 43), ('away', 41), ('wont', 41), ('one', 41), ('day', 40), ('leave', 39), ('home', 39), ('want', 38)]\n",
      "There are 5198 tokens in the data.\n",
      "There are 1075 unique tokens in the data.\n",
      "There are 24596 characters in the data.\n",
      "The lexical diversity is 0.207 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 125), ('love', 77), ('see', 72), ('im', 66), ('ive', 64), ('let', 63), ('youre', 61), ('nothing', 57), ('ill', 52), ('go', 51), ('say', 47), ('oh', 46), ('feel', 45), ('time', 44), ('away', 41), ('wont', 41), ('one', 41), ('want', 40), ('day', 40), ('leave', 39)]\n",
      "There are 5225 tokens in the data.\n",
      "There are 1079 unique tokens in the data.\n",
      "There are 24714 characters in the data.\n",
      "The lexical diversity is 0.207 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 125), ('love', 77), ('see', 73), ('im', 67), ('ive', 64), ('let', 63), ('youre', 61), ('nothing', 57), ('ill', 52), ('go', 51), ('say', 47), ('oh', 47), ('feel', 45), ('time', 44), ('away', 41), ('wont', 41), ('one', 41), ('want', 40), ('home', 40), ('day', 40)]\n",
      "There are 5270 tokens in the data.\n",
      "There are 1088 unique tokens in the data.\n",
      "There are 24926 characters in the data.\n",
      "The lexical diversity is 0.206 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 125), ('love', 77), ('see', 73), ('im', 67), ('ive', 64), ('let', 63), ('youre', 61), ('nothing', 57), ('ill', 52), ('go', 52), ('say', 47), ('oh', 47), ('feel', 45), ('time', 44), ('home', 44), ('away', 41), ('wont', 41), ('one', 41), ('want', 40), ('day', 40)]\n",
      "There are 5412 tokens in the data.\n",
      "There are 1101 unique tokens in the data.\n",
      "There are 25634 characters in the data.\n",
      "The lexical diversity is 0.203 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 125), ('love', 85), ('im', 77), ('see', 73), ('ive', 67), ('let', 63), ('youre', 61), ('nothing', 57), ('say', 56), ('ill', 52), ('go', 52), ('home', 47), ('oh', 47), ('feel', 45), ('one', 45), ('time', 44), ('away', 41), ('wont', 41), ('want', 40), ('day', 40)]\n",
      "There are 5512 tokens in the data.\n",
      "There are 1105 unique tokens in the data.\n",
      "There are 26157 characters in the data.\n",
      "The lexical diversity is 0.200 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 128), ('love', 102), ('im', 78), ('see', 75), ('ive', 67), ('let', 63), ('youre', 61), ('nothing', 57), ('say', 56), ('ill', 54), ('go', 54), ('home', 47), ('oh', 47), ('feel', 46), ('one', 45), ('time', 44), ('away', 41), ('wont', 41), ('want', 40), ('day', 40)]\n",
      "There are 5583 tokens in the data.\n",
      "There are 1125 unique tokens in the data.\n",
      "There are 26555 characters in the data.\n",
      "The lexical diversity is 0.202 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 128), ('love', 102), ('im', 78), ('see', 75), ('ive', 67), ('let', 63), ('youre', 61), ('nothing', 57), ('say', 56), ('ill', 54), ('go', 54), ('oh', 49), ('home', 47), ('feel', 46), ('one', 45), ('time', 44), ('away', 41), ('wont', 41), ('want', 40), ('day', 40)]\n",
      "There are 5745 tokens in the data.\n",
      "There are 1139 unique tokens in the data.\n",
      "There are 27320 characters in the data.\n",
      "The lexical diversity is 0.198 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 129), ('love', 111), ('im', 84), ('see', 75), ('ive', 71), ('let', 63), ('youre', 61), ('say', 58), ('ill', 57), ('nothing', 57), ('go', 55), ('home', 51), ('oh', 49), ('feel', 46), ('one', 46), ('wont', 45), ('time', 44), ('away', 41), ('want', 40), ('day', 40)]\n",
      "There are 5886 tokens in the data.\n",
      "There are 1178 unique tokens in the data.\n",
      "There are 27981 characters in the data.\n",
      "The lexical diversity is 0.200 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 129), ('love', 112), ('im', 88), ('see', 75), ('ive', 71), ('go', 63), ('let', 63), ('youre', 61), ('say', 58), ('ill', 57), ('nothing', 57), ('home', 51), ('oh', 50), ('feel', 46), ('one', 46), ('wont', 45), ('know', 45), ('time', 44), ('get', 42), ('away', 41)]\n",
      "There are 6003 tokens in the data.\n",
      "There are 1192 unique tokens in the data.\n",
      "There are 28506 characters in the data.\n",
      "The lexical diversity is 0.199 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 129), ('love', 112), ('im', 88), ('see', 75), ('ive', 71), ('go', 64), ('let', 63), ('youre', 61), ('ill', 58), ('say', 58), ('nothing', 57), ('time', 56), ('home', 51), ('oh', 50), ('know', 49), ('one', 47), ('feel', 46), ('wont', 45), ('get', 42), ('away', 41)]\n",
      "There are 6106 tokens in the data.\n",
      "There are 1197 unique tokens in the data.\n",
      "There are 28999 characters in the data.\n",
      "The lexical diversity is 0.196 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 129), ('love', 112), ('im', 90), ('see', 76), ('ive', 73), ('go', 66), ('let', 63), ('youre', 61), ('ill', 58), ('say', 58), ('nothing', 57), ('time', 56), ('home', 51), ('know', 50), ('oh', 50), ('feel', 47), ('one', 47), ('wont', 46), ('want', 43), ('without', 43)]\n",
      "There are 6229 tokens in the data.\n",
      "There are 1208 unique tokens in the data.\n",
      "There are 29555 characters in the data.\n",
      "The lexical diversity is 0.194 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 131), ('love', 112), ('im', 92), ('see', 82), ('ive', 75), ('go', 66), ('let', 63), ('youre', 61), ('time', 60), ('ill', 59), ('say', 58), ('nothing', 57), ('home', 52), ('know', 51), ('oh', 50), ('one', 48), ('feel', 47), ('away', 46), ('wont', 46), ('want', 44)]\n",
      "There are 6385 tokens in the data.\n",
      "There are 1218 unique tokens in the data.\n",
      "There are 30284 characters in the data.\n",
      "The lexical diversity is 0.191 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 135), ('love', 112), ('see', 101), ('im', 93), ('ive', 75), ('go', 66), ('youre', 64), ('let', 63), ('time', 60), ('ill', 59), ('say', 58), ('nothing', 57), ('home', 52), ('know', 52), ('oh', 50), ('one', 49), ('want', 47), ('feel', 47), ('away', 46), ('wont', 46)]\n",
      "There are 6497 tokens in the data.\n",
      "There are 1234 unique tokens in the data.\n",
      "There are 30828 characters in the data.\n",
      "The lexical diversity is 0.190 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 135), ('love', 112), ('see', 103), ('im', 95), ('ive', 81), ('go', 66), ('youre', 65), ('let', 63), ('time', 61), ('ill', 60), ('say', 58), ('nothing', 57), ('know', 53), ('home', 52), ('one', 51), ('oh', 50), ('want', 48), ('wont', 47), ('feel', 47), ('away', 46)]\n",
      "There are 6580 tokens in the data.\n",
      "There are 1248 unique tokens in the data.\n",
      "There are 31251 characters in the data.\n",
      "The lexical diversity is 0.190 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 136), ('love', 113), ('see', 103), ('im', 95), ('ive', 81), ('go', 67), ('youre', 65), ('let', 63), ('time', 61), ('ill', 60), ('say', 60), ('nothing', 57), ('know', 53), ('home', 52), ('one', 51), ('want', 50), ('oh', 50), ('wont', 47), ('leave', 47), ('feel', 47)]\n",
      "There are 6672 tokens in the data.\n",
      "There are 1262 unique tokens in the data.\n",
      "There are 31747 characters in the data.\n",
      "The lexical diversity is 0.189 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 140), ('love', 113), ('see', 103), ('im', 95), ('ive', 81), ('youre', 69), ('go', 67), ('time', 65), ('let', 63), ('ill', 60), ('say', 60), ('nothing', 57), ('know', 53), ('home', 52), ('one', 51), ('want', 50), ('oh', 50), ('wont', 47), ('leave', 47), ('feel', 47)]\n",
      "There are 6822 tokens in the data.\n",
      "There are 1273 unique tokens in the data.\n",
      "There are 32467 characters in the data.\n",
      "The lexical diversity is 0.187 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 144), ('love', 140), ('see', 103), ('im', 95), ('ive', 81), ('youre', 69), ('go', 67), ('time', 65), ('let', 63), ('ill', 60), ('say', 60), ('nothing', 57), ('know', 55), ('home', 52), ('one', 51), ('want', 50), ('oh', 50), ('feel', 49), ('wont', 47), ('leave', 47)]\n",
      "There are 6964 tokens in the data.\n",
      "There are 1284 unique tokens in the data.\n",
      "There are 33129 characters in the data.\n",
      "The lexical diversity is 0.184 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 144), ('love', 141), ('see', 103), ('im', 102), ('ive', 85), ('youre', 75), ('go', 68), ('time', 65), ('let', 64), ('ill', 60), ('say', 60), ('nothing', 57), ('know', 56), ('away', 52), ('home', 52), ('one', 51), ('want', 50), ('oh', 50), ('back', 49), ('feel', 49)]\n",
      "There are 7064 tokens in the data.\n",
      "There are 1295 unique tokens in the data.\n",
      "There are 33588 characters in the data.\n",
      "The lexical diversity is 0.183 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 144), ('love', 143), ('see', 104), ('im', 104), ('ive', 85), ('youre', 77), ('go', 70), ('time', 65), ('let', 65), ('ill', 61), ('say', 60), ('home', 58), ('nothing', 57), ('know', 56), ('away', 54), ('oh', 53), ('feel', 52), ('one', 52), ('back', 50), ('want', 50)]\n",
      "There are 7173 tokens in the data.\n",
      "There are 1309 unique tokens in the data.\n",
      "There are 34066 characters in the data.\n",
      "The lexical diversity is 0.182 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 150), ('love', 143), ('im', 108), ('see', 104), ('ive', 85), ('youre', 77), ('go', 75), ('time', 65), ('let', 65), ('ill', 61), ('say', 60), ('home', 59), ('nothing', 57), ('know', 56), ('back', 55), ('want', 55), ('away', 54), ('oh', 53), ('feel', 53), ('one', 52)]\n",
      "There are 7248 tokens in the data.\n",
      "There are 1313 unique tokens in the data.\n",
      "There are 34459 characters in the data.\n",
      "The lexical diversity is 0.181 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 150), ('love', 144), ('im', 108), ('see', 104), ('ive', 85), ('youre', 77), ('go', 75), ('time', 65), ('home', 65), ('let', 65), ('ill', 61), ('say', 60), ('nothing', 57), ('know', 56), ('back', 55), ('want', 55), ('away', 54), ('oh', 53), ('feel', 53), ('one', 52)]\n",
      "There are 7355 tokens in the data.\n",
      "There are 1318 unique tokens in the data.\n",
      "There are 34932 characters in the data.\n",
      "The lexical diversity is 0.179 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 150), ('love', 144), ('im', 117), ('see', 105), ('ive', 85), ('youre', 83), ('go', 75), ('time', 67), ('home', 65), ('let', 65), ('ill', 64), ('say', 60), ('know', 58), ('feel', 58), ('nothing', 57), ('back', 55), ('want', 55), ('away', 54), ('oh', 54), ('one', 52)]\n",
      "There are 7464 tokens in the data.\n",
      "There are 1324 unique tokens in the data.\n",
      "There are 35397 characters in the data.\n",
      "The lexical diversity is 0.177 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 150), ('love', 145), ('im', 117), ('see', 106), ('ive', 85), ('youre', 83), ('home', 80), ('go', 75), ('time', 67), ('let', 65), ('ill', 64), ('say', 60), ('know', 58), ('feel', 58), ('nothing', 57), ('back', 55), ('want', 55), ('away', 55), ('oh', 54), ('one', 52)]\n",
      "There are 7511 tokens in the data.\n",
      "There are 1345 unique tokens in the data.\n",
      "There are 35646 characters in the data.\n",
      "The lexical diversity is 0.179 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 150), ('love', 145), ('im', 117), ('see', 106), ('ive', 85), ('youre', 83), ('home', 80), ('go', 75), ('time', 68), ('let', 65), ('ill', 64), ('say', 60), ('know', 58), ('feel', 58), ('nothing', 57), ('back', 55), ('want', 55), ('away', 55), ('oh', 54), ('one', 53)]\n",
      "There are 7608 tokens in the data.\n",
      "There are 1357 unique tokens in the data.\n",
      "There are 36053 characters in the data.\n",
      "The lexical diversity is 0.178 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 150), ('love', 145), ('im', 122), ('see', 108), ('ive', 85), ('youre', 84), ('home', 81), ('go', 75), ('time', 68), ('let', 65), ('ill', 64), ('say', 60), ('want', 58), ('know', 58), ('feel', 58), ('oh', 57), ('nothing', 57), ('back', 55), ('away', 55), ('day', 55)]\n",
      "There are 7703 tokens in the data.\n",
      "There are 1368 unique tokens in the data.\n",
      "There are 36515 characters in the data.\n",
      "The lexical diversity is 0.178 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 151), ('dont', 150), ('im', 122), ('see', 108), ('ive', 87), ('youre', 84), ('home', 81), ('go', 75), ('time', 70), ('let', 65), ('ill', 64), ('say', 62), ('day', 60), ('know', 59), ('want', 58), ('feel', 58), ('oh', 57), ('nothing', 57), ('away', 56), ('back', 55)]\n",
      "There are 7764 tokens in the data.\n",
      "There are 1371 unique tokens in the data.\n",
      "There are 36748 characters in the data.\n",
      "The lexical diversity is 0.177 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 162), ('dont', 152), ('im', 122), ('see', 108), ('ive', 87), ('youre', 84), ('home', 81), ('go', 75), ('oh', 71), ('time', 70), ('let', 65), ('ill', 64), ('day', 63), ('say', 62), ('know', 59), ('feel', 59), ('want', 58), ('away', 57), ('nothing', 57), ('back', 55)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7854 tokens in the data.\n",
      "There are 1389 unique tokens in the data.\n",
      "There are 37282 characters in the data.\n",
      "The lexical diversity is 0.177 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 162), ('dont', 152), ('im', 122), ('see', 108), ('ive', 87), ('youre', 84), ('home', 82), ('go', 75), ('oh', 73), ('time', 70), ('let', 70), ('ill', 64), ('day', 64), ('say', 62), ('know', 59), ('feel', 59), ('want', 58), ('away', 57), ('nothing', 57), ('back', 55)]\n",
      "There are 7988 tokens in the data.\n",
      "There are 1392 unique tokens in the data.\n",
      "There are 37978 characters in the data.\n",
      "The lexical diversity is 0.174 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 164), ('dont', 152), ('im', 122), ('see', 108), ('ive', 87), ('never', 85), ('youre', 84), ('home', 82), ('go', 75), ('oh', 73), ('time', 70), ('let', 70), ('enough', 66), ('ill', 64), ('day', 64), ('say', 62), ('know', 60), ('feel', 59), ('want', 58), ('away', 57)]\n",
      "There are 8061 tokens in the data.\n",
      "There are 1396 unique tokens in the data.\n",
      "There are 38357 characters in the data.\n",
      "The lexical diversity is 0.173 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 166), ('dont', 154), ('im', 122), ('see', 108), ('ive', 87), ('never', 85), ('youre', 84), ('home', 82), ('go', 75), ('time', 74), ('oh', 73), ('let', 70), ('enough', 66), ('ill', 64), ('day', 64), ('say', 62), ('know', 60), ('feel', 59), ('want', 58), ('away', 57)]\n",
      "There are 8151 tokens in the data.\n",
      "There are 1405 unique tokens in the data.\n",
      "There are 38729 characters in the data.\n",
      "The lexical diversity is 0.172 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 166), ('dont', 154), ('im', 123), ('see', 108), ('ive', 89), ('never', 86), ('youre', 84), ('home', 83), ('go', 77), ('let', 75), ('time', 74), ('oh', 73), ('day', 66), ('enough', 66), ('ill', 64), ('say', 62), ('know', 60), ('feel', 59), ('want', 58), ('away', 57)]\n",
      "There are 8282 tokens in the data.\n",
      "There are 1407 unique tokens in the data.\n",
      "There are 39382 characters in the data.\n",
      "The lexical diversity is 0.170 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 166), ('dont', 161), ('im', 126), ('see', 108), ('never', 96), ('ive', 89), ('youre', 84), ('home', 83), ('go', 77), ('let', 75), ('time', 74), ('oh', 73), ('day', 66), ('enough', 66), ('ill', 64), ('say', 62), ('know', 60), ('feel', 59), ('want', 58), ('away', 57)]\n",
      "There are 8381 tokens in the data.\n",
      "There are 1417 unique tokens in the data.\n",
      "There are 39865 characters in the data.\n",
      "The lexical diversity is 0.169 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 166), ('dont', 162), ('im', 128), ('see', 108), ('youre', 98), ('never', 97), ('ive', 92), ('home', 85), ('go', 78), ('time', 77), ('let', 75), ('oh', 73), ('know', 68), ('day', 66), ('enough', 66), ('ill', 64), ('say', 62), ('feel', 59), ('back', 58), ('want', 58)]\n",
      "There are 8493 tokens in the data.\n",
      "There are 1425 unique tokens in the data.\n",
      "There are 40370 characters in the data.\n",
      "The lexical diversity is 0.168 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 172), ('dont', 162), ('im', 136), ('see', 108), ('youre', 98), ('never', 97), ('ive', 93), ('home', 85), ('go', 83), ('time', 77), ('let', 76), ('oh', 73), ('know', 70), ('day', 66), ('enough', 66), ('ill', 65), ('say', 63), ('back', 59), ('feel', 59), ('want', 58)]\n",
      "There are 8613 tokens in the data.\n",
      "There are 1431 unique tokens in the data.\n",
      "There are 40896 characters in the data.\n",
      "The lexical diversity is 0.166 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 173), ('dont', 167), ('im', 137), ('see', 108), ('youre', 98), ('never', 97), ('ive', 93), ('home', 86), ('go', 83), ('time', 77), ('oh', 76), ('let', 76), ('feel', 75), ('know', 70), ('say', 69), ('day', 66), ('enough', 66), ('ill', 65), ('back', 59), ('away', 59)]\n",
      "There are 8684 tokens in the data.\n",
      "There are 1434 unique tokens in the data.\n",
      "There are 41229 characters in the data.\n",
      "The lexical diversity is 0.165 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 177), ('dont', 170), ('im', 137), ('see', 108), ('youre', 101), ('never', 97), ('ive', 93), ('home', 86), ('go', 83), ('time', 78), ('let', 77), ('know', 76), ('oh', 76), ('feel', 75), ('say', 69), ('ill', 66), ('day', 66), ('enough', 66), ('back', 59), ('away', 59)]\n",
      "There are 8768 tokens in the data.\n",
      "There are 1444 unique tokens in the data.\n",
      "There are 41633 characters in the data.\n",
      "The lexical diversity is 0.165 in the data.\n",
      "The ten most common tokens are:\n",
      "[('dont', 179), ('love', 177), ('im', 141), ('see', 110), ('youre', 101), ('never', 97), ('ive', 93), ('home', 86), ('go', 83), ('time', 78), ('let', 77), ('know', 76), ('oh', 76), ('feel', 76), ('say', 69), ('enough', 67), ('ill', 66), ('day', 66), ('need', 63), ('nothing', 60)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter()\n",
    "song_length = []\n",
    "for column in dido_files_df:\n",
    "    descriptive_stats2(dido_files_df[column], top_x_tokens = 20, verbose=True) \n",
    "dido_song_length = song_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4addd2d4",
   "metadata": {},
   "source": [
    "## Above & Beyond Lyrics -- Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 206 characters in the data.\n",
      "The lexical diversity is 0.200 in the data.\n",
      "The ten most common tokens are:\n",
      "[('need', 12), ('air', 12), ('forever', 4), ('whateverthat', 4), ('lifewe', 2), ('breath', 2), ('lifenow', 2), ('soul', 2)]\n",
      "There are 86 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 439 characters in the data.\n",
      "The lexical diversity is 0.535 in the data.\n",
      "The ten most common tokens are:\n",
      "[('need', 12), ('air', 12), ('love', 5), ('forever', 4), ('whateverthat', 4), ('dont', 3), ('lifewe', 2), ('breath', 2), ('lifenow', 2), ('soul', 2), ('alchemy', 2), ('take', 2), ('words', 1), ('speak', 1), ('foreign', 1), ('land', 1), ('youre', 1), ('telling', 1), ('birds', 1), ('fly', 1)]\n",
      "There are 132 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 667 characters in the data.\n",
      "The lexical diversity is 0.538 in the data.\n",
      "The ten most common tokens are:\n",
      "[('need', 12), ('air', 12), ('world', 11), ('call', 6), ('love', 5), ('forever', 4), ('whateverthat', 4), ('dont', 3), ('lifewe', 2), ('breath', 2), ('lifenow', 2), ('soul', 2), ('alchemy', 2), ('got', 2), ('take', 2), ('maybe', 2), ('lost', 2), ('wind', 2), ('soar', 2), ('tried', 2)]\n",
      "There are 244 tokens in the data.\n",
      "There are 117 unique tokens in the data.\n",
      "There are 1191 characters in the data.\n",
      "The lexical diversity is 0.480 in the data.\n",
      "The ten most common tokens are:\n",
      "[('way', 22), ('need', 14), ('air', 12), ('world', 11), ('call', 6), ('love', 5), ('forever', 4), ('whateverthat', 4), ('home', 4), ('im', 4), ('talked', 4), ('much', 4), ('couldnt', 4), ('hear', 4), ('soul', 3), ('dont', 3), ('things', 3), ('looked', 3), ('know', 3), ('almost', 3)]\n",
      "There are 382 tokens in the data.\n",
      "There are 150 unique tokens in the data.\n",
      "There are 1986 characters in the data.\n",
      "The lexical diversity is 0.393 in the data.\n",
      "The ten most common tokens are:\n",
      "[('way', 22), ('need', 14), ('tonight', 13), ('air', 12), ('alone', 12), ('world', 11), ('im', 8), ('love', 7), ('wouldnt', 7), ('call', 6), ('like', 6), ('angels', 6), ('blind', 6), ('forever', 4), ('whateverthat', 4), ('home', 4), ('talked', 4), ('much', 4), ('couldnt', 4), ('hear', 4)]\n",
      "There are 482 tokens in the data.\n",
      "There are 190 unique tokens in the data.\n",
      "There are 2458 characters in the data.\n",
      "The lexical diversity is 0.394 in the data.\n",
      "The ten most common tokens are:\n",
      "[('way', 22), ('im', 21), ('need', 14), ('alone', 13), ('tonight', 13), ('air', 12), ('gonna', 12), ('alright', 12), ('world', 11), ('love', 8), ('wouldnt', 7), ('call', 6), ('like', 6), ('angels', 6), ('blind', 6), ('ive', 5), ('couldnt', 5), ('forever', 4), ('whateverthat', 4), ('take', 4)]\n",
      "There are 603 tokens in the data.\n",
      "There are 196 unique tokens in the data.\n",
      "There are 3006 characters in the data.\n",
      "The lexical diversity is 0.325 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 40), ('way', 22), ('alright', 22), ('going', 16), ('need', 14), ('alone', 14), ('gonna', 14), ('tonight', 13), ('air', 12), ('world', 11), ('love', 10), ('take', 8), ('time', 8), ('make', 7), ('ive', 7), ('wouldnt', 7), ('call', 6), ('lost', 6), ('couldnt', 6), ('like', 6)]\n",
      "There are 699 tokens in the data.\n",
      "There are 213 unique tokens in the data.\n",
      "There are 3469 characters in the data.\n",
      "The lexical diversity is 0.305 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 40), ('way', 26), ('always', 23), ('alright', 22), ('going', 16), ('need', 15), ('alone', 14), ('gonna', 14), ('tonight', 13), ('air', 12), ('world', 11), ('love', 10), ('take', 9), ('time', 9), ('make', 7), ('things', 7), ('maybe', 7), ('ive', 7), ('wouldnt', 7), ('call', 6)]\n",
      "There are 779 tokens in the data.\n",
      "There are 221 unique tokens in the data.\n",
      "There are 3806 characters in the data.\n",
      "The lexical diversity is 0.284 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 40), ('way', 26), ('always', 23), ('alright', 22), ('time', 19), ('going', 16), ('need', 15), ('love', 15), ('alone', 14), ('gonna', 14), ('come', 14), ('know', 13), ('tonight', 13), ('air', 12), ('world', 11), ('take', 9), ('hold', 9), ('wont', 8), ('past', 8), ('make', 7)]\n",
      "There are 924 tokens in the data.\n",
      "There are 267 unique tokens in the data.\n",
      "There are 4630 characters in the data.\n",
      "The lexical diversity is 0.289 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 40), ('way', 26), ('always', 25), ('alright', 22), ('come', 21), ('time', 19), ('going', 16), ('need', 15), ('love', 15), ('bittersweet', 15), ('blue', 15), ('alone', 14), ('gonna', 14), ('know', 13), ('tonight', 13), ('air', 12), ('like', 12), ('life', 11), ('world', 11), ('take', 10)]\n",
      "There are 1098 tokens in the data.\n",
      "There are 327 unique tokens in the data.\n",
      "There are 5462 characters in the data.\n",
      "The lexical diversity is 0.298 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 40), ('way', 29), ('always', 25), ('alright', 22), ('come', 21), ('time', 20), ('love', 17), ('going', 16), ('need', 15), ('youre', 15), ('bittersweet', 15), ('blue', 15), ('alone', 14), ('gonna', 14), ('world', 13), ('know', 13), ('tonight', 13), ('room', 13), ('air', 12), ('like', 12)]\n",
      "There are 1205 tokens in the data.\n",
      "There are 346 unique tokens in the data.\n",
      "There are 5928 characters in the data.\n",
      "The lexical diversity is 0.287 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 52), ('way', 29), ('always', 25), ('alright', 22), ('come', 21), ('time', 20), ('blue', 19), ('day', 18), ('love', 17), ('gonna', 17), ('know', 16), ('going', 16), ('need', 15), ('youre', 15), ('bittersweet', 15), ('world', 14), ('alone', 14), ('tonight', 13), ('room', 13), ('air', 12)]\n",
      "There are 1332 tokens in the data.\n",
      "There are 380 unique tokens in the data.\n",
      "There are 6522 characters in the data.\n",
      "The lexical diversity is 0.285 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 53), ('way', 29), ('always', 25), ('cant', 23), ('youre', 22), ('alright', 22), ('come', 21), ('time', 20), ('blue', 19), ('love', 18), ('day', 18), ('sleep', 18), ('away', 17), ('gonna', 17), ('know', 16), ('going', 16), ('need', 15), ('bittersweet', 15), ('world', 14), ('alone', 14)]\n",
      "There are 1411 tokens in the data.\n",
      "There are 403 unique tokens in the data.\n",
      "There are 6902 characters in the data.\n",
      "The lexical diversity is 0.286 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 55), ('way', 31), ('always', 25), ('cant', 23), ('youre', 22), ('alright', 22), ('come', 21), ('time', 20), ('blue', 19), ('love', 18), ('know', 18), ('day', 18), ('sleep', 18), ('ive', 17), ('away', 17), ('gonna', 17), ('going', 16), ('need', 15), ('bittersweet', 15), ('world', 14)]\n",
      "There are 1557 tokens in the data.\n",
      "There are 429 unique tokens in the data.\n",
      "There are 7566 characters in the data.\n",
      "The lexical diversity is 0.276 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 55), ('way', 31), ('always', 26), ('love', 23), ('know', 23), ('cant', 23), ('youre', 22), ('alright', 22), ('come', 21), ('time', 20), ('away', 19), ('blue', 19), ('day', 18), ('sleep', 18), ('dont', 17), ('ive', 17), ('gonna', 17), ('going', 16), ('want', 16), ('need', 15)]\n",
      "There are 1644 tokens in the data.\n",
      "There are 441 unique tokens in the data.\n",
      "There are 7996 characters in the data.\n",
      "The lexical diversity is 0.268 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 55), ('way', 31), ('always', 26), ('love', 25), ('youre', 24), ('know', 23), ('cant', 23), ('time', 22), ('alright', 22), ('come', 21), ('ive', 20), ('away', 19), ('gonna', 19), ('blue', 19), ('day', 18), ('sleep', 18), ('dont', 17), ('going', 16), ('want', 16), ('need', 15)]\n",
      "There are 1695 tokens in the data.\n",
      "There are 446 unique tokens in the data.\n",
      "There are 8256 characters in the data.\n",
      "The lexical diversity is 0.263 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 55), ('love', 32), ('way', 32), ('always', 26), ('youre', 24), ('know', 23), ('cant', 23), ('time', 22), ('alright', 22), ('come', 21), ('diving', 21), ('ive', 20), ('gonna', 20), ('away', 19), ('day', 19), ('blue', 19), ('sleep', 18), ('dont', 17), ('going', 16), ('want', 16)]\n",
      "There are 1768 tokens in the data.\n",
      "There are 476 unique tokens in the data.\n",
      "There are 8623 characters in the data.\n",
      "The lexical diversity is 0.269 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 58), ('love', 32), ('way', 32), ('always', 26), ('youre', 25), ('cant', 24), ('know', 23), ('time', 23), ('ive', 22), ('alright', 22), ('come', 22), ('diving', 21), ('gonna', 20), ('away', 19), ('day', 19), ('blue', 19), ('dont', 18), ('sleep', 18), ('world', 17), ('going', 17)]\n",
      "There are 1890 tokens in the data.\n",
      "There are 507 unique tokens in the data.\n",
      "There are 9229 characters in the data.\n",
      "The lexical diversity is 0.268 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 58), ('way', 33), ('youre', 32), ('love', 32), ('always', 26), ('cant', 24), ('know', 23), ('time', 23), ('ive', 22), ('alright', 22), ('come', 22), ('gonna', 21), ('diving', 21), ('away', 19), ('day', 19), ('blue', 19), ('dont', 18), ('sleep', 18), ('world', 17), ('like', 17)]\n",
      "There are 1947 tokens in the data.\n",
      "There are 519 unique tokens in the data.\n",
      "There are 9510 characters in the data.\n",
      "The lexical diversity is 0.267 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 58), ('love', 36), ('way', 33), ('youre', 32), ('time', 27), ('always', 26), ('cant', 24), ('know', 23), ('dont', 22), ('ive', 22), ('alright', 22), ('come', 22), ('gonna', 21), ('diving', 21), ('blue', 20), ('away', 19), ('day', 19), ('sleep', 18), ('world', 17), ('like', 17)]\n",
      "There are 2047 tokens in the data.\n",
      "There are 529 unique tokens in the data.\n",
      "There are 10040 characters in the data.\n",
      "The lexical diversity is 0.258 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 58), ('love', 36), ('youre', 35), ('way', 35), ('time', 27), ('always', 26), ('know', 24), ('cant', 24), ('ive', 23), ('dont', 22), ('alright', 22), ('come', 22), ('away', 21), ('gonna', 21), ('diving', 21), ('blue', 20), ('day', 19), ('ever', 19), ('sleep', 18), ('world', 17)]\n",
      "There are 2176 tokens in the data.\n",
      "There are 556 unique tokens in the data.\n",
      "There are 10657 characters in the data.\n",
      "The lexical diversity is 0.256 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 58), ('love', 37), ('youre', 36), ('way', 36), ('know', 28), ('time', 28), ('dont', 26), ('always', 26), ('cant', 24), ('ive', 23), ('alright', 22), ('come', 22), ('away', 21), ('gonna', 21), ('diving', 21), ('day', 20), ('blue', 20), ('ever', 19), ('like', 18), ('sleep', 18)]\n",
      "There are 2196 tokens in the data.\n",
      "There are 559 unique tokens in the data.\n",
      "There are 10725 characters in the data.\n",
      "The lexical diversity is 0.255 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 58), ('love', 37), ('youre', 36), ('way', 36), ('know', 28), ('time', 28), ('dont', 26), ('always', 26), ('cant', 24), ('ive', 23), ('alright', 22), ('come', 22), ('away', 21), ('gonna', 21), ('diving', 21), ('day', 20), ('blue', 20), ('ever', 19), ('like', 18), ('sleep', 18)]\n",
      "There are 2253 tokens in the data.\n",
      "There are 568 unique tokens in the data.\n",
      "There are 10995 characters in the data.\n",
      "The lexical diversity is 0.252 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 60), ('love', 38), ('youre', 36), ('way', 36), ('know', 28), ('time', 28), ('dont', 26), ('ive', 26), ('always', 26), ('cant', 24), ('come', 23), ('alright', 22), ('away', 21), ('gonna', 21), ('diving', 21), ('day', 20), ('blue', 20), ('going', 19), ('want', 19), ('ever', 19)]\n",
      "There are 2287 tokens in the data.\n",
      "There are 569 unique tokens in the data.\n",
      "There are 11137 characters in the data.\n",
      "The lexical diversity is 0.249 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 60), ('youre', 40), ('love', 40), ('way', 36), ('know', 30), ('time', 28), ('dont', 26), ('ive', 26), ('always', 26), ('cant', 24), ('come', 23), ('alright', 22), ('away', 21), ('gonna', 21), ('diving', 21), ('day', 20), ('blue', 20), ('going', 19), ('want', 19), ('ever', 19)]\n",
      "There are 2324 tokens in the data.\n",
      "There are 579 unique tokens in the data.\n",
      "There are 11316 characters in the data.\n",
      "The lexical diversity is 0.249 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 60), ('love', 41), ('youre', 40), ('way', 36), ('know', 33), ('time', 28), ('dont', 26), ('ive', 26), ('always', 26), ('cant', 24), ('come', 23), ('alright', 22), ('day', 22), ('away', 21), ('gonna', 21), ('diving', 21), ('blue', 20), ('going', 19), ('want', 19), ('ever', 19)]\n",
      "There are 2401 tokens in the data.\n",
      "There are 596 unique tokens in the data.\n",
      "There are 11741 characters in the data.\n",
      "The lexical diversity is 0.248 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 60), ('love', 44), ('youre', 40), ('way', 36), ('know', 33), ('ever', 31), ('time', 28), ('dont', 26), ('ive', 26), ('always', 26), ('cant', 25), ('come', 23), ('alright', 22), ('day', 22), ('life', 21), ('away', 21), ('gonna', 21), ('diving', 21), ('blue', 20), ('going', 19)]\n",
      "There are 2492 tokens in the data.\n",
      "There are 622 unique tokens in the data.\n",
      "There are 12177 characters in the data.\n",
      "The lexical diversity is 0.250 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 62), ('love', 47), ('youre', 41), ('way', 36), ('know', 34), ('ever', 31), ('time', 29), ('dont', 26), ('ive', 26), ('always', 26), ('cant', 26), ('come', 24), ('away', 23), ('alright', 22), ('day', 22), ('life', 21), ('gonna', 21), ('diving', 21), ('like', 20), ('blue', 20)]\n",
      "There are 2534 tokens in the data.\n",
      "There are 624 unique tokens in the data.\n",
      "There are 12375 characters in the data.\n",
      "The lexical diversity is 0.246 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 62), ('love', 54), ('way', 42), ('youre', 41), ('know', 39), ('ever', 31), ('time', 29), ('dont', 28), ('ive', 26), ('always', 26), ('cant', 26), ('come', 24), ('away', 23), ('alright', 22), ('day', 22), ('life', 21), ('gonna', 21), ('diving', 21), ('like', 20), ('blue', 20)]\n",
      "There are 2594 tokens in the data.\n",
      "There are 636 unique tokens in the data.\n",
      "There are 12655 characters in the data.\n",
      "The lexical diversity is 0.245 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 64), ('im', 62), ('way', 42), ('youre', 41), ('know', 39), ('come', 31), ('ever', 31), ('time', 29), ('dont', 28), ('ive', 26), ('always', 26), ('cant', 26), ('away', 23), ('alright', 22), ('day', 22), ('life', 21), ('like', 21), ('gonna', 21), ('diving', 21), ('blue', 20)]\n",
      "There are 2693 tokens in the data.\n",
      "There are 657 unique tokens in the data.\n",
      "There are 13163 characters in the data.\n",
      "The lexical diversity is 0.244 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 65), ('im', 63), ('youre', 47), ('way', 43), ('know', 40), ('time', 31), ('come', 31), ('ever', 31), ('dont', 28), ('ive', 26), ('always', 26), ('cant', 26), ('like', 25), ('life', 23), ('away', 23), ('day', 23), ('alright', 22), ('gonna', 21), ('diving', 21), ('blue', 20)]\n",
      "There are 2714 tokens in the data.\n",
      "There are 661 unique tokens in the data.\n",
      "There are 13263 characters in the data.\n",
      "The lexical diversity is 0.244 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 66), ('love', 65), ('youre', 47), ('way', 45), ('know', 40), ('time', 31), ('come', 31), ('ever', 31), ('dont', 28), ('ive', 26), ('always', 26), ('cant', 26), ('like', 25), ('life', 23), ('away', 23), ('day', 23), ('alright', 22), ('gonna', 21), ('diving', 21), ('blue', 20)]\n",
      "There are 2764 tokens in the data.\n",
      "There are 670 unique tokens in the data.\n",
      "There are 13494 characters in the data.\n",
      "The lexical diversity is 0.242 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 71), ('im', 66), ('youre', 47), ('way', 45), ('know', 42), ('time', 32), ('come', 31), ('ever', 31), ('dont', 30), ('ive', 28), ('always', 26), ('cant', 26), ('like', 25), ('day', 25), ('away', 24), ('life', 23), ('alright', 22), ('gonna', 21), ('diving', 21), ('blue', 20)]\n",
      "There are 2840 tokens in the data.\n",
      "There are 696 unique tokens in the data.\n",
      "There are 13916 characters in the data.\n",
      "The lexical diversity is 0.245 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 71), ('im', 66), ('youre', 47), ('way', 45), ('know', 42), ('time', 37), ('come', 31), ('ever', 31), ('dont', 30), ('ive', 28), ('always', 26), ('cant', 26), ('like', 25), ('day', 25), ('away', 24), ('life', 23), ('alright', 22), ('gonna', 21), ('diving', 21), ('blue', 20)]\n",
      "There are 2938 tokens in the data.\n",
      "There are 721 unique tokens in the data.\n",
      "There are 14476 characters in the data.\n",
      "The lexical diversity is 0.245 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 71), ('im', 66), ('youre', 47), ('way', 45), ('know', 44), ('time', 38), ('dont', 31), ('come', 31), ('ever', 31), ('ive', 28), ('always', 26), ('away', 26), ('cant', 26), ('like', 25), ('day', 25), ('life', 23), ('alright', 22), ('gonna', 21), ('diving', 21), ('going', 20)]\n",
      "There are 3024 tokens in the data.\n",
      "There are 742 unique tokens in the data.\n",
      "There are 14868 characters in the data.\n",
      "The lexical diversity is 0.245 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 71), ('im', 66), ('youre', 47), ('way', 45), ('know', 44), ('time', 39), ('dont', 31), ('come', 31), ('ever', 31), ('ive', 28), ('always', 26), ('away', 26), ('cant', 26), ('like', 25), ('day', 25), ('life', 23), ('alright', 22), ('good', 22), ('gonna', 21), ('diving', 21)]\n",
      "There are 3120 tokens in the data.\n",
      "There are 744 unique tokens in the data.\n",
      "There are 15289 characters in the data.\n",
      "The lexical diversity is 0.238 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 71), ('im', 66), ('youre', 47), ('way', 45), ('know', 44), ('time', 40), ('dont', 31), ('come', 31), ('ever', 31), ('good', 29), ('ive', 28), ('always', 26), ('away', 26), ('cant', 26), ('hymn', 26), ('like', 25), ('day', 25), ('life', 23), ('alright', 22), ('gonna', 21)]\n",
      "There are 3223 tokens in the data.\n",
      "There are 753 unique tokens in the data.\n",
      "There are 15755 characters in the data.\n",
      "The lexical diversity is 0.234 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 73), ('im', 68), ('youre', 47), ('know', 47), ('way', 45), ('time', 40), ('dont', 31), ('come', 31), ('ever', 31), ('ive', 29), ('good', 29), ('get', 29), ('life', 27), ('always', 26), ('away', 26), ('cant', 26), ('hymn', 26), ('like', 25), ('day', 25), ('alright', 22)]\n",
      "There are 3294 tokens in the data.\n",
      "There are 760 unique tokens in the data.\n",
      "There are 16042 characters in the data.\n",
      "The lexical diversity is 0.231 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 75), ('im', 70), ('youre', 50), ('know', 47), ('way', 45), ('time', 42), ('come', 33), ('dont', 31), ('like', 31), ('ever', 31), ('ive', 30), ('good', 29), ('get', 29), ('life', 28), ('always', 26), ('away', 26), ('cant', 26), ('hymn', 26), ('day', 25), ('alright', 22)]\n",
      "There are 3416 tokens in the data.\n",
      "There are 794 unique tokens in the data.\n",
      "There are 16644 characters in the data.\n",
      "The lexical diversity is 0.232 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 75), ('im', 71), ('youre', 50), ('know', 47), ('way', 45), ('time', 42), ('take', 35), ('come', 33), ('dont', 31), ('ive', 31), ('like', 31), ('ever', 31), ('soul', 30), ('good', 29), ('get', 29), ('life', 28), ('always', 26), ('away', 26), ('cant', 26), ('hymn', 26)]\n",
      "There are 3505 tokens in the data.\n",
      "There are 809 unique tokens in the data.\n",
      "There are 17057 characters in the data.\n",
      "The lexical diversity is 0.231 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 75), ('im', 74), ('know', 51), ('youre', 50), ('way', 45), ('time', 42), ('take', 35), ('like', 35), ('ive', 33), ('come', 33), ('dont', 31), ('good', 31), ('ever', 31), ('soul', 30), ('get', 29), ('life', 28), ('day', 27), ('always', 26), ('away', 26), ('cant', 26)]\n",
      "There are 3571 tokens in the data.\n",
      "There are 819 unique tokens in the data.\n",
      "There are 17371 characters in the data.\n",
      "The lexical diversity is 0.229 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 77), ('im', 74), ('know', 53), ('youre', 52), ('way', 45), ('time', 42), ('take', 35), ('like', 35), ('dont', 34), ('ive', 33), ('come', 33), ('good', 31), ('ever', 31), ('soul', 30), ('life', 29), ('get', 29), ('day', 27), ('always', 26), ('away', 26), ('cant', 26)]\n",
      "There are 3717 tokens in the data.\n",
      "There are 844 unique tokens in the data.\n",
      "There are 18062 characters in the data.\n",
      "The lexical diversity is 0.227 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 85), ('love', 78), ('know', 56), ('youre', 54), ('way', 49), ('time', 42), ('take', 35), ('like', 35), ('dont', 34), ('ive', 33), ('come', 33), ('always', 32), ('get', 32), ('good', 31), ('ever', 31), ('soul', 30), ('life', 29), ('day', 27), ('want', 27), ('away', 26)]\n",
      "There are 3800 tokens in the data.\n",
      "There are 854 unique tokens in the data.\n",
      "There are 18461 characters in the data.\n",
      "The lexical diversity is 0.225 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 85), ('love', 78), ('know', 56), ('youre', 54), ('way', 50), ('time', 46), ('ever', 36), ('take', 35), ('like', 35), ('dont', 34), ('life', 33), ('ive', 33), ('come', 33), ('always', 32), ('get', 32), ('good', 31), ('soul', 30), ('day', 27), ('want', 27), ('away', 26)]\n",
      "There are 3861 tokens in the data.\n",
      "There are 864 unique tokens in the data.\n",
      "There are 18748 characters in the data.\n",
      "The lexical diversity is 0.224 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 85), ('love', 79), ('know', 56), ('youre', 54), ('way', 50), ('time', 46), ('ever', 42), ('dont', 40), ('like', 36), ('take', 35), ('life', 33), ('ive', 33), ('come', 33), ('always', 32), ('get', 32), ('good', 31), ('soul', 30), ('day', 27), ('want', 27), ('away', 26)]\n",
      "There are 3942 tokens in the data.\n",
      "There are 874 unique tokens in the data.\n",
      "There are 19165 characters in the data.\n",
      "The lexical diversity is 0.222 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 88), ('im', 85), ('youre', 56), ('know', 56), ('way', 51), ('time', 46), ('ever', 42), ('dont', 40), ('like', 36), ('take', 35), ('life', 33), ('ive', 33), ('come', 33), ('always', 32), ('get', 32), ('good', 31), ('soul', 30), ('day', 27), ('want', 27), ('away', 26)]\n",
      "There are 4051 tokens in the data.\n",
      "There are 896 unique tokens in the data.\n",
      "There are 19735 characters in the data.\n",
      "The lexical diversity is 0.221 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 90), ('im', 88), ('youre', 59), ('know', 57), ('way', 51), ('time', 47), ('ever', 42), ('dont', 40), ('like', 37), ('take', 35), ('life', 34), ('come', 34), ('ive', 33), ('always', 32), ('get', 32), ('soul', 31), ('good', 31), ('away', 29), ('day', 27), ('want', 27)]\n",
      "There are 4106 tokens in the data.\n",
      "There are 906 unique tokens in the data.\n",
      "There are 19989 characters in the data.\n",
      "The lexical diversity is 0.221 in the data.\n",
      "The ten most common tokens are:\n",
      "[('love', 90), ('im', 88), ('youre', 59), ('know', 57), ('way', 51), ('time', 47), ('ever', 42), ('dont', 40), ('like', 37), ('come', 36), ('take', 35), ('get', 35), ('life', 34), ('ive', 33), ('always', 32), ('soul', 31), ('good', 31), ('away', 29), ('day', 27), ('want', 27)]\n",
      "There are 4203 tokens in the data.\n",
      "There are 915 unique tokens in the data.\n",
      "There are 20419 characters in the data.\n",
      "The lexical diversity is 0.218 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 92), ('love', 90), ('youre', 71), ('know', 61), ('way', 51), ('time', 47), ('ever', 42), ('dont', 40), ('ive', 37), ('like', 37), ('come', 36), ('take', 35), ('get', 35), ('life', 34), ('always', 32), ('soul', 31), ('good', 31), ('away', 30), ('never', 30), ('day', 27)]\n",
      "There are 4297 tokens in the data.\n",
      "There are 939 unique tokens in the data.\n",
      "There are 20948 characters in the data.\n",
      "The lexical diversity is 0.219 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 92), ('love', 90), ('youre', 71), ('know', 61), ('way', 51), ('time', 47), ('dont', 44), ('ever', 42), ('ive', 37), ('like', 37), ('come', 36), ('take', 35), ('get', 35), ('life', 34), ('always', 32), ('soul', 31), ('good', 31), ('away', 30), ('never', 30), ('day', 27)]\n",
      "There are 4392 tokens in the data.\n",
      "There are 955 unique tokens in the data.\n",
      "There are 21383 characters in the data.\n",
      "The lexical diversity is 0.217 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 92), ('love', 90), ('youre', 71), ('know', 61), ('way', 51), ('time', 47), ('dont', 44), ('ever', 42), ('take', 41), ('like', 38), ('ive', 37), ('come', 36), ('get', 35), ('life', 34), ('always', 32), ('soul', 31), ('away', 31), ('good', 31), ('never', 30), ('day', 27)]\n",
      "There are 4432 tokens in the data.\n",
      "There are 957 unique tokens in the data.\n",
      "There are 21575 characters in the data.\n",
      "The lexical diversity is 0.216 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 97), ('love', 90), ('youre', 71), ('know', 62), ('way', 51), ('time', 49), ('dont', 44), ('ever', 42), ('take', 41), ('like', 38), ('ive', 37), ('life', 36), ('come', 36), ('get', 35), ('always', 32), ('soul', 31), ('away', 31), ('good', 31), ('never', 30), ('day', 27)]\n",
      "There are 4520 tokens in the data.\n",
      "There are 969 unique tokens in the data.\n",
      "There are 22039 characters in the data.\n",
      "The lexical diversity is 0.214 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 98), ('love', 90), ('youre', 71), ('know', 62), ('get', 52), ('way', 51), ('time', 49), ('dont', 44), ('ever', 42), ('take', 41), ('like', 39), ('ive', 37), ('life', 36), ('come', 36), ('always', 32), ('soul', 31), ('away', 31), ('good', 31), ('never', 30), ('want', 28)]\n",
      "There are 4623 tokens in the data.\n",
      "There are 975 unique tokens in the data.\n",
      "There are 22565 characters in the data.\n",
      "The lexical diversity is 0.211 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 101), ('love', 90), ('youre', 71), ('get', 68), ('know', 62), ('way', 51), ('time', 50), ('dont', 44), ('ever', 42), ('take', 41), ('head', 41), ('like', 40), ('ive', 37), ('life', 36), ('come', 36), ('sticky', 33), ('fingers', 33), ('always', 32), ('soul', 31), ('away', 31)]\n",
      "There are 4696 tokens in the data.\n",
      "There are 987 unique tokens in the data.\n",
      "There are 22887 characters in the data.\n",
      "The lexical diversity is 0.210 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 104), ('love', 90), ('get', 72), ('youre', 71), ('know', 64), ('way', 51), ('time', 50), ('dont', 46), ('ever', 42), ('take', 41), ('head', 41), ('like', 40), ('never', 38), ('ive', 37), ('life', 36), ('come', 36), ('fingers', 35), ('sticky', 33), ('always', 32), ('soul', 31)]\n",
      "There are 4698 tokens in the data.\n",
      "There are 988 unique tokens in the data.\n",
      "There are 22900 characters in the data.\n",
      "The lexical diversity is 0.210 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 104), ('love', 90), ('get', 72), ('youre', 71), ('know', 64), ('way', 51), ('time', 50), ('dont', 46), ('ever', 42), ('take', 41), ('head', 41), ('like', 40), ('never', 38), ('ive', 37), ('life', 36), ('come', 36), ('fingers', 35), ('sticky', 33), ('always', 32), ('soul', 31)]\n",
      "There are 4787 tokens in the data.\n",
      "There are 1001 unique tokens in the data.\n",
      "There are 23341 characters in the data.\n",
      "The lexical diversity is 0.209 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 104), ('love', 98), ('get', 72), ('youre', 71), ('know', 66), ('way', 51), ('time', 51), ('dont', 50), ('ever', 42), ('take', 41), ('like', 41), ('head', 41), ('never', 39), ('life', 38), ('ive', 37), ('come', 36), ('fingers', 35), ('sticky', 33), ('always', 32), ('soul', 31)]\n",
      "There are 4860 tokens in the data.\n",
      "There are 1009 unique tokens in the data.\n",
      "There are 23715 characters in the data.\n",
      "The lexical diversity is 0.208 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 104), ('love', 98), ('know', 79), ('youre', 74), ('get', 72), ('way', 51), ('time', 51), ('dont', 50), ('ever', 42), ('take', 41), ('like', 41), ('head', 41), ('never', 39), ('life', 38), ('ive', 37), ('come', 36), ('fingers', 35), ('sticky', 33), ('always', 32), ('soul', 31)]\n",
      "There are 4900 tokens in the data.\n",
      "There are 1014 unique tokens in the data.\n",
      "There are 23949 characters in the data.\n",
      "The lexical diversity is 0.207 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 104), ('love', 100), ('know', 79), ('youre', 74), ('get', 72), ('way', 51), ('time', 51), ('dont', 50), ('ever', 42), ('take', 41), ('like', 41), ('head', 41), ('never', 39), ('life', 38), ('ive', 37), ('come', 36), ('fingers', 35), ('sticky', 33), ('always', 32), ('soul', 31)]\n",
      "There are 4986 tokens in the data.\n",
      "There are 1029 unique tokens in the data.\n",
      "There are 24340 characters in the data.\n",
      "The lexical diversity is 0.206 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 106), ('love', 101), ('know', 79), ('youre', 74), ('get', 72), ('way', 51), ('time', 51), ('dont', 50), ('like', 42), ('ever', 42), ('take', 41), ('head', 41), ('never', 39), ('life', 38), ('need', 37), ('ive', 37), ('come', 37), ('fingers', 35), ('always', 33), ('sticky', 33)]\n",
      "There are 5021 tokens in the data.\n",
      "There are 1035 unique tokens in the data.\n",
      "There are 24547 characters in the data.\n",
      "The lexical diversity is 0.206 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 106), ('love', 101), ('know', 79), ('youre', 75), ('get', 72), ('way', 51), ('time', 51), ('dont', 50), ('like', 42), ('ever', 42), ('take', 41), ('head', 41), ('never', 39), ('life', 38), ('need', 37), ('ive', 37), ('come', 37), ('fingers', 35), ('always', 33), ('sticky', 33)]\n",
      "There are 5136 tokens in the data.\n",
      "There are 1042 unique tokens in the data.\n",
      "There are 25068 characters in the data.\n",
      "The lexical diversity is 0.203 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 106), ('love', 101), ('know', 85), ('youre', 75), ('get', 72), ('way', 51), ('time', 51), ('dont', 50), ('like', 42), ('ever', 42), ('take', 41), ('go', 41), ('head', 41), ('need', 40), ('got', 39), ('never', 39), ('life', 38), ('ive', 37), ('come', 37), ('fingers', 35)]\n",
      "There are 5176 tokens in the data.\n",
      "There are 1042 unique tokens in the data.\n",
      "There are 25256 characters in the data.\n",
      "The lexical diversity is 0.201 in the data.\n",
      "The ten most common tokens are:\n",
      "[('im', 106), ('love', 101), ('know', 87), ('youre', 75), ('get', 72), ('way', 51), ('time', 51), ('dont', 50), ('got', 45), ('go', 43), ('like', 42), ('ever', 42), ('need', 41), ('take', 41), ('head', 41), ('never', 39), ('life', 38), ('ive', 37), ('come', 37), ('fingers', 35)]\n"
     ]
    }
   ],
   "source": [
    "counter=Counter()\n",
    "song_length = []\n",
    "for column in ab_files_df:\n",
    "    descriptive_stats2(ab_files_df[column], top_x_tokens = 20, verbose=True) \n",
    "ab_song_length = song_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: Leaving in stopwords would not eliminate words like \"the\" \"a\" and \"an\" and these would likely appear in the list of the most frequent words.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: I would have expected a bit more diversity, but the same words: \"I'm\", \"love\", \"know\", \"you're\", \"get\", \"don't\", \"need\", \"go\" and so on dominate the top words.  Even though I know Above and Beyond is a Trance/DJ band, and Dido is Pop/Solo Artist performer, I could not have distingusihed them from this analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f75b0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package emoji:\n",
      "\n",
      "NAME\n",
      "    emoji\n",
      "\n",
      "DESCRIPTION\n",
      "    emoji for Python\n",
      "    ~~~~~~~~~~~~~~~~\n",
      "    \n",
      "    emoji terminal output for Python.\n",
      "    \n",
      "        >>> import emoji\n",
      "        >>> print(emoji.emojize('Python is :thumbsup:', language='alias'))\n",
      "        Python is 👍\n",
      "        >>> print(emoji.emojize('Python is :thumbs_up:'))\n",
      "        Python is 👍\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    core\n",
      "    unicode_codes (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    demojize(string, delimiters=(':', ':'), language='en', version=None, handle_version=None)\n",
      "        Replace Unicode emoji in a string with emoji shortcodes. Useful for storage.\n",
      "            >>> import emoji\n",
      "            >>> print(emoji.emojize(\"Python is fun :thumbs_up:\"))\n",
      "            Python is fun 👍\n",
      "            >>> print(emoji.demojize(u\"Python is fun 👍\"))\n",
      "            Python is fun :thumbs_up:\n",
      "            >>> print(emoji.demojize(u\"Unicode is tricky 😯\", delimiters=(\"__\", \"__\")))\n",
      "            Unicode is tricky __hushed_face__\n",
      "        \n",
      "        :param string: String contains Unicode characters. MUST BE UNICODE.\n",
      "        :param delimiters: (optional) User delimiters other than ``_DEFAULT_DELIMITER``\n",
      "        :param language: Choose language of emoji name: language code 'es', 'de', etc. or 'alias'\n",
      "            to use English aliases\n",
      "        :param version: (optional) Max version. If set to an Emoji Version,\n",
      "            all emoji above this version will be removed.\n",
      "        :param handle_version: (optional) Replace the emoji above ``version``\n",
      "            instead of removing it. handle_version can be either a string or a\n",
      "            callable ``handle_version(emj: str, data: dict) -> str``; If it is\n",
      "            a callable, it's passed the Unicode emoji and the data dict from\n",
      "            emoji.EMOJI_DATA and must return a replacement string  to be used.\n",
      "            The passed data is in the form of::\n",
      "        \n",
      "                handle_version(u'\\U0001F6EB', {\n",
      "                    'en' : ':airplane_departure:',\n",
      "                    'status' : fully_qualified,\n",
      "                    'E' : 1,\n",
      "                    'alias' : [u':flight_departure:'],\n",
      "                    'de': u':abflug:',\n",
      "                    'es': u':avión_despegando:',\n",
      "                    ...\n",
      "                })\n",
      "    \n",
      "    distinct_emoji_list(string)\n",
      "        Returns distinct list of emojis from the string.\n",
      "    \n",
      "    emoji_count(string, unique=False)\n",
      "        Returns the count of emojis in a string.\n",
      "        \n",
      "        :param unique: (optional) True if count only unique emojis\n",
      "    \n",
      "    emoji_list(string)\n",
      "        Returns the location and emoji in list of dict format.\n",
      "            >>> emoji.emoji_list(\"Hi, I am fine. 😁\")\n",
      "            [{'match_start': 15, 'match_end': 16, 'emoji': '😁'}]\n",
      "    \n",
      "    emojize(string, delimiters=(':', ':'), variant=None, language='en', version=None, handle_version=None)\n",
      "        Replace emoji names in a string with Unicode codes.\n",
      "            >>> import emoji\n",
      "            >>> print(emoji.emojize(\"Python is fun :thumbsup:\", language='alias'))\n",
      "            Python is fun 👍\n",
      "            >>> print(emoji.emojize(\"Python is fun :thumbs_up:\"))\n",
      "            Python is fun 👍\n",
      "            >>> print(emoji.emojize(\"Python is fun {thumbs_up}\", delimiters = (\"{\", \"}\")))\n",
      "            Python is fun 👍\n",
      "            >>> print(emoji.emojize(\"Python is fun :red_heart:\", variant=\"text_type\"))\n",
      "            Python is fun ❤\n",
      "            >>> print(emoji.emojize(\"Python is fun :red_heart:\", variant=\"emoji_type\"))\n",
      "            Python is fun ❤️ # red heart, not black heart\n",
      "        \n",
      "        :param string: String contains emoji names.\n",
      "        :param delimiters: (optional) Use delimiters other than _DEFAULT_DELIMITER. Each delimiter\n",
      "            should contain at least one character that is not part of a-zA-Z0-9 and ``_-–&.’”“()!?#*+,/\\``\n",
      "        :param variant: (optional) Choose variation selector between \"base\"(None), VS-15 (\"text_type\") and VS-16 (\"emoji_type\")\n",
      "        :param language: Choose language of emoji name: language code 'es', 'de', etc. or 'alias'\n",
      "            to use English aliases\n",
      "        :param version: (optional) Max version. If set to an Emoji Version,\n",
      "            all emoji above this version will be ignored.\n",
      "        :param handle_version: (optional) Replace the emoji above ``version``\n",
      "            instead of ignoring it. handle_version can be either a string or a\n",
      "            callable; If it is a callable, it's passed the Unicode emoji and the\n",
      "            data dict from emoji.EMOJI_DATA and must return a replacement string\n",
      "            to be used::\n",
      "        \n",
      "                handle_version(u'\\U0001F6EB', {\n",
      "                    'en' : ':airplane_departure:',\n",
      "                    'status' : fully_qualified,\n",
      "                    'E' : 1,\n",
      "                    'alias' : [u':flight_departure:'],\n",
      "                    'de': u':abflug:',\n",
      "                    'es': u':avión_despegando:',\n",
      "                    ...\n",
      "                })\n",
      "        \n",
      "        :raises ValueError: if ``variant`` is neither None, 'text_type' or 'emoji_type'\n",
      "    \n",
      "    is_emoji(string)\n",
      "        Returns True if the string is an emoji.\n",
      "    \n",
      "    replace_emoji(string, replace='', version=-1)\n",
      "        Replace Unicode emoji in a customizable string.\n",
      "        \n",
      "        :param string: String contains Unicode characters. MUST BE UNICODE.\n",
      "        :param replace: (optional) replace can be either a string or a callable;\n",
      "            If it is a callable, it's passed the Unicode emoji and the data dict from\n",
      "            emoji.EMOJI_DATA and must return a replacement string to be used.\n",
      "            replace(str, dict) -> str\n",
      "        :param version: (optional) Max version. If set to an Emoji Version,\n",
      "            only emoji above this version will be replaced.\n",
      "    \n",
      "    version(string)\n",
      "        Returns the Emoji Version of the emoji.\n",
      "        \n",
      "        See http://www.unicode.org/reports/tr51/#Versioning for more information.\n",
      "            >>> emoji.version(\"😁\")\n",
      "            0.6\n",
      "            >>> emoji.version(\":butterfly:\")\n",
      "            3\n",
      "        \n",
      "        :param string: An emoji or a text containig an emoji\n",
      "        :raises ValueError: if ``string`` does not contain an emoji\n",
      "\n",
      "DATA\n",
      "    EMOJI_DATA = {'#⃣': {'E': 0.6, 'alias': [':hash:'], 'en': ':keycap_#:'...\n",
      "    LANGUAGES = ['en', 'es', 'pt', 'it', 'fr', 'de', 'fa']\n",
      "    STATUS = {'component': 1, 'fully_qualified': 2, 'minimally_qualified':...\n",
      "    __all__ = ['emojize', 'demojize', 'emoji_count', 'emoji_list', 'distin...\n",
      "    __email__ = 'carpedm20@gmail.com'\n",
      "    __license__ = '\\nNew BSD License\\n\\nCopyright (c) 2014-2022, Taeho...E...\n",
      "    __source__ = 'https://github.com/carpedm20/emoji/'\n",
      "\n",
      "VERSION\n",
      "    2.1.0\n",
      "\n",
      "AUTHOR\n",
      "    Taehoon Kim, Kevin Wurster and Tahir Jalilov\n",
      "\n",
      "FILE\n",
      "    c:\\users\\drzag\\anaconda3\\lib\\site-packages\\emoji\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "aea1356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is fun 👍\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(emoji.emojize(\"Python is fun :thumbs_up:\"))\n",
    "from emoji import is_emoji\n",
    "print(is_emoji('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [174]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(is_emoji(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:-)\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(is_emoji(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❤️\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(is_emoji(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:-)\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def is_emoji(s):\n",
    "#    return(s in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "print(is_emoji(\"❤️\"))\n",
    "print(is_emoji(\":-)\"))\n",
    "\n",
    "assert(is_emoji(\"❤️\"))\n",
    "assert(is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "269cd433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "♥ 561\n",
      "❤️ 451\n",
      "❤ 302\n",
      "🏳️‍🌈 241\n",
      "🎶 179\n",
      "✨ 162\n",
      "💙 145\n",
      "🇲🇽 128\n",
      "💜 103\n",
      "🌈 98\n",
      "🇬🇧 87\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "dido_fol['assignment'].map(counter.update)\n",
    "num=0\n",
    "for key, value in counter.most_common():\n",
    "    if num <11:\n",
    "        if is_emoji(key) is True:\n",
    "            print(key, value)\n",
    "            num+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2eafea70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❤️ 507\n",
      "🎶 393\n",
      "🎧 368\n",
      "✨ 333\n",
      "🇲🇽 325\n",
      "🏳️‍🌈 313\n",
      "❤ 266\n",
      "😎 249\n",
      "🖤 193\n",
      "💜 175\n",
      "💙 172\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "ab_fol['assignment'].map(counter.update)\n",
    "num=0\n",
    "for key, value in counter.most_common():\n",
    "    if num <11:\n",
    "        if is_emoji(key) is True:\n",
    "            print(key, value)\n",
    "            num+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#music 126\n",
      "# 80\n",
      "#trancefamily 59\n",
      "#blacklivesmatter 58\n",
      "#1 44\n",
      "#mufc 43\n",
      "#ynwa 40\n",
      "#blm 38\n",
      "#love 37\n",
      "#art 34\n",
      "#trance 34\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "dido_fol['with_hashes'].map(counter.update)\n",
    "num=0\n",
    "for key, value in counter.most_common():\n",
    "    if num <11:\n",
    "        if key.startswith('#'):\n",
    "            print(key, value)\n",
    "            num+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a6158786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#trance 187\n",
      "#edm 178\n",
      "#music 123\n",
      "#trancefamily 108\n",
      "#bitcoin 107\n",
      "#techno 106\n",
      "#blacklivesmatter 94\n",
      "#dj 85\n",
      "#blm 81\n",
      "# 79\n",
      "#house 78\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "ab_fol['with_hashes'].map(counter.update)\n",
    "num=0\n",
    "for key, value in counter.most_common():\n",
    "    if num <11:\n",
    "        if key.startswith('#'):\n",
    "            print(key, value)\n",
    "            num+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love 7\n",
      "day 4\n",
      "dont 4\n",
      "see 3\n",
      "want 2\n",
      "home 2\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "dido_titles_df['assignment'].map(counter.update)\n",
    "num=0\n",
    "for key, value in counter.most_common():\n",
    "    if num <6:\n",
    "        print(key, value)\n",
    "        num+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "afe6bc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love 8\n",
      "home 3\n",
      "good 3\n",
      "alone 2\n",
      "tonight 2\n",
      "alright 2\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "ab_titles_df['assignment'].map(counter.update)\n",
    "num=0\n",
    "for key, value in counter.most_common():\n",
    "    if num <6:\n",
    "        print(key, value)\n",
    "        num+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8db66e48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 96, 104, 102, 78, 79, 130, 83, 121, 122, 121, 104, 124, 133, 111, 91, 172, 123, 128, 111, 154, 114, 92, 132, 98, 96, 47, 40, 100, 83, 89, 114, 90, 113, 88, 103, 156, 92, 210, 138, 89, 33, 103, 55, 102, 119, 104, 113, 27, 45, 142, 100, 71, 162, 141, 117, 103, 123, 156, 112, 83, 92, 150, 142, 100, 109, 75, 107, 109, 47, 97, 95, 61, 90, 134, 73, 90, 131, 99, 112, 120, 71, 84]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(dido_song_length)\n",
    "plt.hist(dido_song_length, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e15ba338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 46, 46, 112, 138, 100, 121, 96, 80, 145, 174, 107, 127, 79, 146, 87, 51, 73, 122, 57, 100, 129, 20, 57, 34, 37, 77, 91, 42, 60, 99, 21, 50, 76, 98, 86, 96, 103, 71, 122, 89, 66, 146, 83, 61, 81, 109, 55, 97, 94, 95, 40, 88, 103, 73, 2, 89, 73, 40, 86, 35, 115, 40]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(ab_song_length)\n",
    "plt.hist(dido_song_length, alpha=0.5, bins=10, label = 'dido lengths')\n",
    "plt.hist(ab_song_length, alpha=0.5, bins=10, label = 'above and beyond lengths')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: One or more whitespace characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your lyric length comparison chart here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
